{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99800bfb-0998-4f0e-90f9-ae99046d56ff",
   "metadata": {
    "id": "99800bfb-0998-4f0e-90f9-ae99046d56ff"
   },
   "source": [
    "# Key Phrases Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe9c93b-c69d-47fe-969b-4c392c11e93a",
   "metadata": {
    "id": "5fe9c93b-c69d-47fe-969b-4c392c11e93a"
   },
   "source": [
    "1. Extractor Keywords\n",
    "   1. Remove Stop Words:\n",
    "      1. Chinese (zh): jieba\n",
    "      2. Arabic (ar): Hugging Face (asafaya/bert-base-arabic)\n",
    "      3. Hindi (hi): indic-nlp-library\n",
    "      4. Basque (eu): tokenize - xx_ent_wiki_sm, Stopwords-iso - stopwords-eu.txt\n",
    "      5. Czech (cs): stopwordsios\n",
    "      6. Farsi (fa): Hazm\n",
    "      7. Other Languages: spaCy model\n",
    "   2. Recognize NER Entities:\n",
    "      1. Hugging Face Models:\n",
    "         1. Arabic (ar): asafaya/bert-base-arabic\n",
    "         2. Catalan (ca): projecte-aina/roberta-base-ca-v2-cased-ner\n",
    "         3. Farsi (fa): HooshvareLab/bert-fa-base-uncased-ner-arman\n",
    "         4. Other Languages: FacebookAI/xlm-roberta-large-finetuned-conll03-english\n",
    "      3. For Unrecognized Content, Perform Tokenization (Extract Key Nouns if Possible):\n",
    "         1. Chinese (zh): jieba (tfidf-keywords)\n",
    "         2. Hindi (hi): indic_tokenize\n",
    "         3. Arabic (ar): Hugging Face (asafaya/bert-base-arabic)\n",
    "         4. Czech (cs): Stanza\n",
    "         5. Farsi (fa): Stanza\n",
    "         6. Other Languages: spaCy tokenize\n",
    "2. Acquire External Knowledgesults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ba63c-e6b8-4f35-b96e-24421a164ca0",
   "metadata": {
    "id": "029ba63c-e6b8-4f35-b96e-24421a164ca0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from hazm import Normalizer, WordTokenizer, stopwords_list\n",
    "import stanza\n",
    "from stopwordsiso import stopwords\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import jieba.analyse\n",
    "import os\n",
    "import torch\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gc\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5d250-2e51-4885-a3f1-6d8843aa741d",
   "metadata": {
    "id": "06d5d250-2e51-4885-a3f1-6d8843aa741d"
   },
   "outputs": [],
   "source": [
    "# proxy setting (if possible)\n",
    "# Jalynn`s settings\n",
    "proxies = {\n",
    "    \"http\": \"http://127.0.0.1:10809\",\n",
    "    \"https\": \"http://127.0.0.1:10809\"\n",
    "}\n",
    "os.environ[\"http_proxy\"] = proxies[\"http\"]\n",
    "os.environ[\"https_proxy\"] = proxies[\"https\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa811341-589f-47a1-854a-76a8a19cafac",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "97076a1a173a4e7cac0cdf7b9c984388",
      "8fd69dc4702e4d02a27276e45548095f"
     ]
    },
    "id": "aa811341-589f-47a1-854a-76a8a19cafac",
    "outputId": "e84afd3c-9987-4b3f-88a1-24d4e7bcb70f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97076a1a173a4e7cac0cdf7b9c984388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 05:53:30 INFO: Downloaded file to C:\\Users\\hjy\\stanza_resources\\resources.json\n",
      "2025-01-17 05:53:30 INFO: Downloading default packages for language: cs (Czech) ...\n",
      "2025-01-17 05:53:31 INFO: File exists: C:\\Users\\hjy\\stanza_resources\\cs\\default.zip\n",
      "2025-01-17 05:53:31 INFO: Finished downloading models and saved to C:\\Users\\hjy\\stanza_resources\n",
      "2025-01-17 05:53:31 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd69dc4702e4d02a27276e45548095f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 05:53:35 INFO: Downloaded file to C:\\Users\\hjy\\stanza_resources\\resources.json\n",
      "2025-01-17 05:53:35 INFO: Loading these models for language: cs (Czech):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | pdt          |\n",
      "| mwt       | pdt          |\n",
      "| pos       | pdt_nocharlm |\n",
      "| lemma     | pdt_nocharlm |\n",
      "| depparse  | pdt_nocharlm |\n",
      "============================\n",
      "\n",
      "2025-01-17 05:53:35 INFO: Using device: cpu\n",
      "2025-01-17 05:53:35 INFO: Loading: tokenize\n",
      "E:\\conda_pkg\\.conda\\envs\\semeval_env_new\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2025-01-17 05:53:35 INFO: Loading: mwt\n",
      "2025-01-17 05:53:36 INFO: Loading: pos\n",
      "2025-01-17 05:53:37 INFO: Loading: lemma\n",
      "2025-01-17 05:53:39 INFO: Loading: depparse\n",
      "2025-01-17 05:53:40 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('cs')\n",
    "nlp_stanza_czech = stanza.Pipeline('cs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753877d-e52a-49f5-92c8-4596fc7be037",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "514962c940ca43b5818960e3742a07ff",
      "c05b6ad7e7ca40048877637bff21eddb"
     ]
    },
    "id": "5753877d-e52a-49f5-92c8-4596fc7be037",
    "outputId": "91824db9-6301-426f-e26f-e0ff81fb81bb",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514962c940ca43b5818960e3742a07ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 05:53:45 INFO: Downloaded file to C:\\Users\\hjy\\stanza_resources\\resources.json\n",
      "2025-01-17 05:53:45 INFO: Downloading default packages for language: fa (Persian) ...\n",
      "2025-01-17 05:53:46 INFO: File exists: C:\\Users\\hjy\\stanza_resources\\fa\\default.zip\n",
      "2025-01-17 05:53:47 INFO: Finished downloading models and saved to C:\\Users\\hjy\\stanza_resources\n",
      "2025-01-17 05:53:47 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05b6ad7e7ca40048877637bff21eddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 05:53:49 INFO: Downloaded file to C:\\Users\\hjy\\stanza_resources\\resources.json\n",
      "2025-01-17 05:53:50 INFO: Loading these models for language: fa (Persian):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | perdt          |\n",
      "| mwt       | perdt          |\n",
      "| pos       | perdt_charlm   |\n",
      "| lemma     | perdt_nocharlm |\n",
      "| depparse  | perdt_charlm   |\n",
      "| ner       | arman          |\n",
      "==============================\n",
      "\n",
      "2025-01-17 05:53:50 INFO: Using device: cpu\n",
      "2025-01-17 05:53:50 INFO: Loading: tokenize\n",
      "2025-01-17 05:53:50 INFO: Loading: mwt\n",
      "2025-01-17 05:53:50 INFO: Loading: pos\n",
      "2025-01-17 05:53:52 INFO: Loading: lemma\n",
      "2025-01-17 05:53:52 INFO: Loading: depparse\n",
      "2025-01-17 05:53:53 INFO: Loading: ner\n",
      "2025-01-17 05:53:55 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('fa')\n",
    "nlp_stanza_fa = stanza.Pipeline('fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54028413-02d8-4a4d-b0a8-c2c9ac469565",
   "metadata": {
    "id": "54028413-02d8-4a4d-b0a8-c2c9ac469565"
   },
   "outputs": [],
   "source": [
    "def clear_model_cache():\n",
    "    \"\"\"\n",
    "    Clear the model cache to free up memory\n",
    "    \"\"\"\n",
    "    global model_cache\n",
    "    for lang, pipeline in model_cache.items():\n",
    "        del pipeline\n",
    "    model_cache.clear()\n",
    "    gc.collect()\n",
    "    print(\"The model cache has been cleared and memory has been freed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59411698-260d-46f9-886c-350bd087e579",
   "metadata": {
    "id": "59411698-260d-46f9-886c-350bd087e579"
   },
   "outputs": [],
   "source": [
    "# Delayed loading model\n",
    "model_cache = {}\n",
    "\n",
    "def get_ner_pipeline(language):\n",
    "    if language not in model_cache:\n",
    "\n",
    "        clear_model_cache()\n",
    "        if language == \"ar\":\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"asafaya/bert-base-arabic\",\n",
    "                cache_dir=\"../huggingface/\", # load local directory\n",
    "                local_files_only=True,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "\n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                \"asafaya/bert-base-arabic\",\n",
    "                cache_dir=\"../huggingface/\", # load local directory\n",
    "                local_files_only=True,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "\n",
    "        elif language == \"ca\":\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"projecte-aina/roberta-base-ca-v2-cased-ner\",\n",
    "                cache_dir=\"../huggingface/\", # load local directory\n",
    "                local_files_only=True,\n",
    "                resume_download=True,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "\n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                \"projecte-aina/roberta-base-ca-v2-cased-ner\",\n",
    "                cache_dir=\"../huggingface/\", # load local directory\n",
    "                local_files_only=True,\n",
    "                resume_download=True,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "\n",
    "        elif language == \"fa\":\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"HooshvareLab/bert-fa-base-uncased-ner-arman\",\n",
    "                cache_dir=\"../huggingface/\", # load local directory\n",
    "                local_files_only=True,\n",
    "                resume_download=True,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "\n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                \"HooshvareLab/bert-fa-base-uncased-ner-arman\",\n",
    "                cache_dir=\"../huggingface/\", # load local directory\n",
    "                local_files_only=True,\n",
    "                resume_download=True,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"FacebookAI/xlm-roberta-large-finetuned-conll03-english\",\n",
    "                cache_dir=\"../huggingface/\", # load local directory\n",
    "                local_files_only=True,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                \"FacebookAI/xlm-roberta-large-finetuned-conll03-english\",\n",
    "                cache_dir=\"../huggingface/\", # load local directory\n",
    "                local_files_only=True,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "\n",
    "        model_cache[language] = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    return model_cache[language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7b711-42e6-4f1c-86c2-0abb34e5bb09",
   "metadata": {
    "id": "7ab7b711-42e6-4f1c-86c2-0abb34e5bb09"
   },
   "outputs": [],
   "source": [
    "# Initial Hugging Face AR language model\n",
    "tokenizer_ar = AutoTokenizer.from_pretrained(\n",
    "    \"asafaya/bert-base-arabic\",\n",
    "    cache_dir=\"../huggingface/\",\n",
    "    local_files_only=True,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "model_ar = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"asafaya/bert-base-arabic\",\n",
    "    cache_dir=\"../huggingface/\",\n",
    "    local_files_only=True,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "ner_pipeline_ar = pipeline(\"ner\", model=model_ar, tokenizer=tokenizer_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5710a-f7bf-4d9b-b813-22768056e338",
   "metadata": {
    "id": "17a5710a-f7bf-4d9b-b813-22768056e338"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "def remove_arabic_punctuation(text):\n",
    "\n",
    "    arabic_punctuation = r'[؟،؛«»…\"“‘’]'\n",
    "\n",
    "    return re.sub(arabic_punctuation, ' ', text)\n",
    "\n",
    "\n",
    "def load_stopwords(language):\n",
    "    if language == \"zh\":\n",
    "        # Chinese stopwords file 'stopwords_zh.txt'\n",
    "        with open(\"../stopwords/stopwords-zh.txt\", encoding=\"utf-8\") as f:\n",
    "            return set(line.strip() for line in f)\n",
    "    elif language == \"ar\":\n",
    "        # Arabic stopwords file 'stopwords_ar.txt'\n",
    "        with open(\"../stopwords/stopwords-ar.txt\", encoding=\"utf-8\") as f:\n",
    "            return set(line.strip() for line in f)\n",
    "\n",
    "    elif language == \"eu\":\n",
    "        with open('../stopwords/stopwords-eu.txt', encoding='utf-8') as f:\n",
    "            return set(line.strip() for line in f)\n",
    "\n",
    "    else:\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875126d6-d350-4203-b52e-023cdc1feceb",
   "metadata": {
    "id": "875126d6-d350-4203-b52e-023cdc1feceb"
   },
   "outputs": [],
   "source": [
    "# filter stop words\n",
    "def filter_stop_words(text: str, language: str):\n",
    "    load_stopwords_file = load_stopwords(language)\n",
    "\n",
    "    if language == \"zh\":\n",
    "\n",
    "        tokens = jieba.cut(text)\n",
    "        filtered_tokens = [token for token in tokens if token.strip() and token not in load_stopwords_file and token not in string.punctuation]\n",
    "        return \" \".join(filtered_tokens)\n",
    "\n",
    "    elif language == 'ar':\n",
    "        # Remove Arabic punctuation\n",
    "        text = remove_arabic_punctuation(text)\n",
    "\n",
    "        arabic_stopwords = stopwords(\"ar\")\n",
    "\n",
    "        combined_stopwords = arabic_stopwords.union(load_stopwords_file)\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        filtered_tokens = [token for token in tokens if token not in combined_stopwords]\n",
    "\n",
    "        return \" \".join(filtered_tokens)\n",
    "\n",
    "    elif language == \"hi\":\n",
    "        hindi_stopwords = set(stopwords('hi'))\n",
    "\n",
    "        tokens = indic_tokenize.trivial_tokenize(text, lang='hi')\n",
    "        filtered_tokens = [token for token in tokens if token not in hindi_stopwords and token not in string.punctuation]\n",
    "\n",
    "        return \" \".join(filtered_tokens)\n",
    "\n",
    "    elif language == \"eu\":\n",
    "        try:\n",
    "            nlp_basque = spacy.load(\"xx_ent_wiki_sm\")\n",
    "        except OSError:\n",
    "            import os\n",
    "            os.system(\"python -m spacy download xx_ent_wiki_sm\")\n",
    "            nlp_basque = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "        filtered_text = ' '.join([token.text for token in nlp_basque(text) if token.text.lower() not in load_stopwords_file])\n",
    "        return filtered_text\n",
    "\n",
    "    elif language == \"cs\":\n",
    "        czech_stopwords = set(stopwords('cs'))\n",
    "        nlp_czech = spacy.blank(\"cs\")\n",
    "        filtered_tokens = ' '.join([token.text for token in nlp_czech(text) if token.text.lower() not in czech_stopwords])\n",
    "        return filtered_tokens\n",
    "\n",
    "    elif language == \"fa\":\n",
    "        normalizer_fa = Normalizer()\n",
    "        tokenizer_fa = WordTokenizer()\n",
    "        stopwords_fa = set(stopwords_list())\n",
    "\n",
    "        normalized_text = normalizer_fa.normalize(text)\n",
    "        tokens = tokenizer_fa.tokenize(normalized_text)\n",
    "        filtered_text = ' '.join([token for token in tokens if token not in stopwords_fa])\n",
    "        return filtered_text\n",
    "\n",
    "    else:\n",
    "\n",
    "        spacy_model_map = {\n",
    "            \"en\": \"en_core_web_sm\",\n",
    "            \"es\": \"es_core_news_sm\",\n",
    "            \"fr\": \"fr_core_news_sm\",\n",
    "            \"de\": \"de_core_news_sm\",\n",
    "            \"it\": \"it_core_news_sm\",\n",
    "            \"fi\": \"fi_core_news_sm\",\n",
    "            \"sv\": \"sv_core_news_sm\",\n",
    "            \"ca\": \"ca_core_news_sm\"\n",
    "        }\n",
    "\n",
    "        model_name = spacy_model_map.get(language)\n",
    "\n",
    "        try:\n",
    "            nlp = spacy.load(model_name)\n",
    "        except Exception:\n",
    "            raise ValueError(f\"Make sure you have installed {model_name} model！\")\n",
    "\n",
    "        doc = nlp(text)\n",
    "        filtered_tokens = [token.text for token in doc if not token.is_stop and not token.is_punct and token.pos_ in {\"NOUN\", \"PROPN\"}]\n",
    "        return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d502500-476d-4e98-8727-1c878d25d679",
   "metadata": {
    "id": "8d502500-476d-4e98-8727-1c878d25d679"
   },
   "outputs": [],
   "source": [
    "def merge_subwords_preserve_spaces(ner_results, text, language='en'):\n",
    "    \"\"\"\n",
    "    Integration entity consolidation:\n",
    "    - Remove special segmentation markers (▁, Ġ, ##)\n",
    "    - Tag merged entities according to B/I\n",
    "    - Retain space and location information\n",
    "    \"\"\"\n",
    "    merged_entities = []\n",
    "    current_entity = None\n",
    "    current_words = []\n",
    "\n",
    "    split_languages = {'zh', 'ar', 'hi', 'cs', 'eu', 'ca', 'fa'}\n",
    "\n",
    "    for result in ner_results:\n",
    "\n",
    "        word = result['word'].replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\")\n",
    "        entity_type = result['entity'].replace(\"B-\", \"\").replace(\"I-\", \"\")\n",
    "\n",
    "        if (current_entity is None or entity_type != current_entity or\n",
    "            ('▁' in result['word'] and language in split_languages)):\n",
    "\n",
    "            if current_words:\n",
    "                start = current_words[0]['start']\n",
    "                end = current_words[-1]['end']\n",
    "\n",
    "                combined_word = \"\".join([\n",
    "                    res['word'].replace(\"▁\", \" \").replace(\"Ġ\", \"\").replace(\"##\", \"\")\n",
    "                    for res in current_words\n",
    "                ]).strip()\n",
    "\n",
    "                # average confidence\n",
    "                avg_score = sum(res['score'] for res in current_words) / len(current_words)\n",
    "\n",
    "                merged_entities.append({\n",
    "                    \"entity\": current_entity,\n",
    "                    \"word\": combined_word,\n",
    "                    \"score\": avg_score,\n",
    "                    \"start\": start,\n",
    "                    \"end\": end\n",
    "                })\n",
    "\n",
    "            current_entity = entity_type\n",
    "            current_words = [result]\n",
    "        else:\n",
    "            current_words.append(result)\n",
    "\n",
    "    if current_words:\n",
    "        start = current_words[0]['start']\n",
    "        end = current_words[-1]['end']\n",
    "        combined_word = \"\".join([\n",
    "            res['word'].replace(\"▁\", \" \").replace(\"Ġ\", \"\").replace(\"##\", \"\")\n",
    "            for res in current_words\n",
    "        ]).strip()\n",
    "        avg_score = sum(res['score'] for res in current_words) / len(current_words)\n",
    "\n",
    "        merged_entities.append({\n",
    "            \"entity\": current_entity,\n",
    "            \"word\": combined_word,\n",
    "            \"score\": avg_score,\n",
    "            \"start\": start,\n",
    "            \"end\": end\n",
    "        })\n",
    "\n",
    "    return merged_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a8a731-167e-496e-af97-61f71e113f53",
   "metadata": {
    "id": "11a8a731-167e-496e-af97-61f71e113f53"
   },
   "outputs": [],
   "source": [
    "# key phrase extracted function\n",
    "def extract_key_phrases(text: str, language: str = \"en\") -> dict:\n",
    "    \"\"\"\n",
    "    extract key phrase:\n",
    "    - filter stop words\n",
    "    - using Hugging Face extract NER\n",
    "    - for unextracted NER sentences, using different methods to extract key phrase (NOUNS)\n",
    "    \"\"\"\n",
    "    # filter stop words\n",
    "    filtered_text = filter_stop_words(text, language)\n",
    "    ner_pipeline = get_ner_pipeline(language)\n",
    "\n",
    "    # extract NER\n",
    "    if language == \"ar\":\n",
    "        ner_results = ner_pipeline(filtered_text)\n",
    "        merged_entities = merge_subwords_preserve_spaces(ner_results, filtered_text)\n",
    "        ner_entities = [entity['word'] for entity in merged_entities]\n",
    "\n",
    "    elif language == \"eu\":\n",
    "        ner_results = ner_pipeline(filtered_text)\n",
    "        merged_entities = merge_subwords_preserve_spaces(ner_results, filtered_text)\n",
    "        ner_entities = [entity['word'] for entity in merged_entities]\n",
    "\n",
    "    elif language == \"ca\":\n",
    "        ner_results = ner_pipeline(filtered_text)\n",
    "        merged_entities = merge_subwords_preserve_spaces(ner_results, filtered_text)\n",
    "        ner_entities = [entity['word'] for entity in merged_entities]\n",
    "\n",
    "    elif language == \"cs\":\n",
    "        ner_results = ner_pipeline(filtered_text)\n",
    "        merged_entities = merge_subwords_preserve_spaces(ner_results, filtered_text)\n",
    "        ner_entities = [entity['word'] for entity in merged_entities]\n",
    "\n",
    "    elif language == \"fa\":\n",
    "        normalizer_fa = Normalizer()\n",
    "        tokenizer_fa = WordTokenizer()\n",
    "        stopwords_fa = set(stopwords_list())\n",
    "\n",
    "        ner_results = ner_pipeline(filtered_text)\n",
    "        merged_entities = merge_subwords_preserve_spaces(ner_results, filtered_text)\n",
    "        ner_entities = [entity['word'] for entity in merged_entities]\n",
    "\n",
    "    else:\n",
    "        ner_results = ner_pipeline(filtered_text)\n",
    "        merged_entities = merge_subwords_preserve_spaces(ner_results, filtered_text)\n",
    "        ner_entities = [entity['word'] for entity in merged_entities]\n",
    "\n",
    "    # if NER entity has been extracted, then return\n",
    "    if ner_entities:\n",
    "        return {\n",
    "            \"NER_entities\": ner_entities,\n",
    "            \"Additional_phrases\": []\n",
    "        }\n",
    "\n",
    "    # if NER cannot be extracted\n",
    "    # for zh, using jieba\n",
    "    elif language == \"zh\":\n",
    "        if not filtered_text.strip():\n",
    "            return {\n",
    "                    \"NER_entities\": ner_entities,\n",
    "                    \"Additional_phrases\": []\n",
    "                }\n",
    "\n",
    "        tfidf_keywords = jieba.analyse.extract_tags(filtered_text, topK=2, withWeight=False)\n",
    "        if not tfidf_keywords:\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": []\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"NER_entities\": ner_entities,\n",
    "            \"Additional_phrases\": tfidf_keywords\n",
    "        }\n",
    "\n",
    "    # for hi, using indic-nlp-library extract token key phrase\n",
    "    elif language == \"hi\":\n",
    "        if not filtered_text.strip():\n",
    "            return {\n",
    "                    \"NER_entities\": ner_entities,\n",
    "                    \"Additional_phrases\": []\n",
    "                }\n",
    "\n",
    "        tokens = list(indic_tokenize.trivial_tokenize(filtered_text))\n",
    "        if not tokens or all(token in string.punctuation for token in tokens):\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": []\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"NER_entities\": ner_entities,\n",
    "            \"Additional_phrases\": tokens\n",
    "        }\n",
    "\n",
    "    elif language == 'ar':\n",
    "        if not filtered_text.strip():\n",
    "            return {\n",
    "                    \"NER_entities\": ner_entities,\n",
    "                    \"Additional_phrases\": []\n",
    "                }\n",
    "        # using Hugging Face model to tokenize\n",
    "        inputs = tokenizer_ar(filtered_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model_ar(**inputs)\n",
    "\n",
    "        tokens = tokenizer_ar.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "        if not tokens:\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": []\n",
    "            }\n",
    "\n",
    "        noun_tokens = [token for token in tokens if token.isalpha() and token not in string.punctuation]\n",
    "\n",
    "        if len(noun_tokens) < 2:\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": noun_tokens\n",
    "            }\n",
    "\n",
    "        # use TF-IDF extract keywords\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([\" \".join(noun_tokens)])\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "\n",
    "        tfidf_keywords = [\n",
    "            feature_names[i]\n",
    "            for i in tfidf_scores.argsort()[-10:][::-1]\n",
    "        ]\n",
    "        return {\n",
    "            \"NER_entities\": ner_entities,\n",
    "            \"Additional_phrases\": tfidf_keywords\n",
    "        }\n",
    "\n",
    "    elif language == \"eu\":\n",
    "        if not filtered_text.strip():\n",
    "            return {\n",
    "                    \"NER_entities\": ner_entities,\n",
    "                    \"Additional_phrases\": []\n",
    "                }\n",
    "        tokens = [ent.text for ent in nlp_basque(filtered_text).ents]\n",
    "\n",
    "        if not tokens:\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": []\n",
    "            }\n",
    "\n",
    "        else:\n",
    "\n",
    "            tfidf_vectorizer = TfidfVectorizer()\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform([\" \".join(tokens)])\n",
    "            feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "            tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "\n",
    "\n",
    "            tfidf_keywords = [\n",
    "                feature_names[i]\n",
    "                for i in tfidf_scores.argsort()[-10:][::-1]\n",
    "            ]\n",
    "\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": tfidf_keywords\n",
    "            }\n",
    "\n",
    "    elif language == \"cs\":\n",
    "        if not filtered_text.strip():\n",
    "            return {\n",
    "                    \"NER_entities\": ner_entities,\n",
    "                    \"Additional_phrases\": []\n",
    "                }\n",
    "\n",
    "        doc = nlp_stanza_czech(filtered_text)\n",
    "        tokens = [word.text for sent in doc.sentences for word in sent.words if word.upos in [\"NOUN\", \"PROPN\"]]\n",
    "\n",
    "        if not tokens or all(token in string.punctuation for token in tokens):\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": []\n",
    "            }\n",
    "\n",
    "        else:\n",
    "\n",
    "            tfidf_vectorizer = TfidfVectorizer()\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform([\" \".join(tokens)])\n",
    "            feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "            tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "\n",
    "\n",
    "            tfidf_keywords = [\n",
    "                feature_names[i]\n",
    "                for i in tfidf_scores.argsort()[-10:][::-1]\n",
    "            ]\n",
    "\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": tfidf_keywords\n",
    "            }\n",
    "\n",
    "    elif language == \"fa\":\n",
    "        if not filtered_text.strip():\n",
    "            return {\n",
    "                    \"NER_entities\": ner_entities,\n",
    "                    \"Additional_phrases\": []\n",
    "                }\n",
    "        doc = nlp_stanza_fa(filtered_text)\n",
    "        tokens = [word.text for sent in doc.sentences for word in sent.words if word.upos in [\"NOUN\", \"PROPN\"]]\n",
    "\n",
    "        if not tokens or all(token in string.punctuation for token in tokens):\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": []\n",
    "            }\n",
    "\n",
    "        else:\n",
    "\n",
    "            tfidf_vectorizer = TfidfVectorizer()\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform([\" \".join(tokens)])\n",
    "            feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "            tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "\n",
    "\n",
    "            tfidf_keywords = [\n",
    "                feature_names[i]\n",
    "                for i in tfidf_scores.argsort()[-10:][::-1]\n",
    "            ]\n",
    "\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": tfidf_keywords\n",
    "            }\n",
    "\n",
    "    else:\n",
    "        # for other languages, using spaCy extract key phrase\n",
    "        try:\n",
    "\n",
    "            spacy_model_map = {\n",
    "                \"en\": \"en_core_web_sm\",\n",
    "                \"es\": \"es_core_news_sm\",\n",
    "                \"fr\": \"fr_core_news_sm\",\n",
    "                \"de\": \"de_core_news_sm\",\n",
    "                \"it\": \"it_core_news_sm\",\n",
    "                \"fi\": \"fi_core_news_sm\",\n",
    "                \"sv\": \"sv_core_news_sm\",\n",
    "                \"eu\": \"xx_ent_wiki_sm\",\n",
    "                \"ca\": \"ca_core_news_sm\"\n",
    "            }\n",
    "\n",
    "            model_name = spacy_model_map.get(language)\n",
    "\n",
    "            nlp = spacy.load(model_name)\n",
    "        except Exception:\n",
    "            raise ValueError(f\"Make sure installed {language}_core_news_sm model！\")\n",
    "\n",
    "        if not filtered_text.strip():\n",
    "            return {\n",
    "                    \"NER_entities\": ner_entities,\n",
    "                    \"Additional_phrases\": []\n",
    "                }\n",
    "        doc = nlp(filtered_text)\n",
    "        tokens = {token.text for token in doc if token.pos_ in {\"NOUN\", \"PROPN\"}}\n",
    "\n",
    "        if not tokens or all(token in string.punctuation for token in tokens):\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": []\n",
    "            }\n",
    "\n",
    "\n",
    "        if len(tokens) < 2:\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": tokens\n",
    "            }\n",
    "\n",
    "        else:\n",
    "\n",
    "            tfidf_vectorizer = TfidfVectorizer()\n",
    "            print(\"tokens: \", tokens)\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform([\" \".join(tokens)])\n",
    "            feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "            tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "\n",
    "\n",
    "            tfidf_keywords = [\n",
    "                feature_names[i]\n",
    "                for i in tfidf_scores.argsort()[-10:][::-1]\n",
    "            ]\n",
    "\n",
    "            return {\n",
    "                \"NER_entities\": ner_entities,\n",
    "                \"Additional_phrases\": tfidf_keywords\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bedd51d-35bc-4825-a547-858046c44379",
   "metadata": {
    "id": "0bedd51d-35bc-4825-a547-858046c44379"
   },
   "source": [
    "## Apply on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55b36b-5606-427c-9adb-d9688b453e57",
   "metadata": {
    "id": "0a55b36b-5606-427c-9adb-d9688b453e57"
   },
   "outputs": [],
   "source": [
    "input_dir = '../data/val/val/'\n",
    "output_dir = '../data/detect_val/extract_m2/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bdea1d-7e01-49e0-b039-588010f682ab",
   "metadata": {
    "id": "48bdea1d-7e01-49e0-b039-588010f682ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in tqdm(os.listdir(input_dir)):\n",
    "    if filename.endswith('.jsonl'):\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "            for line in infile:\n",
    "                data = json.loads(line)\n",
    "                text = data.get('model_input', '')\n",
    "                language = data.get('lang', 'en').lower()\n",
    "                # print(\"language: \", language)\n",
    "\n",
    "                key_phrases_result = extract_key_phrases(text, language)\n",
    "                ner_entities = key_phrases_result.get(\"NER_entities\", [])\n",
    "                additional_phrases = key_phrases_result.get(\"Additional_phrases\", [])\n",
    "\n",
    "                combined_keywords = list(set(ner_entities + list(additional_phrases)))\n",
    "\n",
    "                data['keywords'] = combined_keywords\n",
    "\n",
    "                outfile.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"Keywords extractiono and save completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5c8cc-ec38-46a5-bf0f-5f9cadcd8cd6",
   "metadata": {
    "id": "9ec5c8cc-ec38-46a5-bf0f-5f9cadcd8cd6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
