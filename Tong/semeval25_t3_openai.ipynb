{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7c6c379-14f0-4423-9f19-5a2d7efee937"
   },
   "source": [
    "# Hallucination Detection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1189959c-2faa-4b3f-8f5b-b8c41cceaf31",
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.476905Z",
     "start_time": "2025-01-19T08:03:15.474560Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import httpx\n",
    "from RefChecker.refchecker.extractor import extractor_prompts\n",
    "# import RefChecker\n",
    "import numpy as np\n",
    "import spacy\n",
    "from scorer import recompute_hard_labels\n",
    "import glob\n",
    "import re\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ],
   "outputs": [],
   "execution_count": 248
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "40d80ff5-4707-404f-ae56-e61a5dbab507",
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.510606Z",
     "start_time": "2025-01-19T08:03:15.502642Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"\")\n"
   ],
   "outputs": [],
   "execution_count": 249
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFOs4S7BY7ui"
   },
   "source": [
    "## 1. Extracting Claims (Extractor) - Each claim is a merger of triple-structured knowledge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f47d942e-0617-40e0-b1a5-1c28f4f6a7d7",
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.520736Z",
     "start_time": "2025-01-19T08:03:15.518677Z"
    }
   },
   "source": [
    "LLM_TRIPLET_EXTRACTION_PROMPT_Q = extractor_prompts.LLM_TRIPLET_EXTRACTION_PROMPT_Q\n",
    "LLM_Triplet_To_Claim_PROMPT_Q = extractor_prompts.LLM_Triplet_To_Claim_PROMPT_Q\n",
    "LLM_CLAIM_EXTRACTION_PROMPT_Q = extractor_prompts.LLM_CLAIM_EXTRACTION_PROMPT_Q"
   ],
   "outputs": [],
   "execution_count": 250
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d59a72f6-e73d-482d-bc9c-e263ef495d27",
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.530649Z",
     "start_time": "2025-01-19T08:03:15.526753Z"
    }
   },
   "source": [
    "def extract_triplets_to_claims(question, model_output_text):\n",
    "    prompt = LLM_CLAIM_EXTRACTION_PROMPT_Q.format(q=question, r=model_output_text)\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI assistant who extracts claims.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "\n",
    "        response_content = chat_completion.choices[0].message.content\n",
    "\n",
    "        if not response_content:\n",
    "            print(f\"No response for the prompt: {prompt}\")\n",
    "            return []\n",
    "\n",
    "        return response_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "        return []\n"
   ],
   "outputs": [],
   "execution_count": 251
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e78c8d47-15c9-4c68-a4b0-ec200591c5fc"
   },
   "source": [
    "## 2. Obtain Complete References"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1a0a7428-4377-4cbb-9a53-6a0df26d48c0",
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.539231Z",
     "start_time": "2025-01-19T08:03:15.536156Z"
    }
   },
   "source": [
    "def get_reference_for_claim(claim):\n",
    "    prompt = f\"\"\"\n",
    "    Please expand, provide additional relevant factual information and verify the following claim:\n",
    "    Claims: {claim}\n",
    "\n",
    "    If the claim is accurate, return the original claim.\n",
    "    If the claim is inaccurate or incomplete, return a corrected, more detailed statement.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI assistant verifying claims.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "\n",
    "        if not chat_completion.choices or len(chat_completion.choices) == 0:\n",
    "            print(f\"No response for the prompt: {prompt}\")\n",
    "            return []\n",
    "\n",
    "        response_content = chat_completion.choices[0].message.content\n",
    "\n",
    "        if not response_content.strip():\n",
    "            print(f\"No content in the response for the prompt: {prompt}\")\n",
    "            return []\n",
    "\n",
    "        return response_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "        return []\n"
   ],
   "outputs": [],
   "execution_count": 252
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9735211d-f550-4a22-b52c-fa523d0faf5c",
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.546629Z",
     "start_time": "2025-01-19T08:03:15.544126Z"
    }
   },
   "source": [
    "def extract_and_get_references(claims, context):\n",
    "    references = []\n",
    "    for claim in claims:\n",
    "        verified_reference = get_reference_for_claim(claim)\n",
    "        references.append(verified_reference)\n",
    "\n",
    "    final_reference = \" \".join(references) + \" \" + context\n",
    "\n",
    "    return final_reference"
   ],
   "outputs": [],
   "execution_count": 253
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de5f64f8-c12d-4bd9-a90e-f5c1ab591941"
   },
   "source": [
    "## 3. Validate claims, `model_input`, `model_output_text`, and References (Checker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a03c1421-e879-41c1-b162-eb72fcc801d0"
   },
   "source": [
    "The validation results should be mapped back to the `model_output_text`, marking hallucination positions and probabilities, and outputting them as `soft_labels`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "76c4340a-731e-4048-8449-0e53fa9b0f24",
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.554110Z",
     "start_time": "2025-01-19T08:03:15.551484Z"
    }
   },
   "source": [
    "def extract_hallucination_positions(model_output_text, hallucination_results):\n",
    "    # parse JSON data\n",
    "    try:\n",
    "        hallucination_results = json.loads(hallucination_results)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to decode JSON. Returning empty labels.\")\n",
    "        return {\"soft_labels\": []}\n",
    "\n",
    "    soft_labels = []\n",
    "\n",
    "    # find the position in the original text\n",
    "    for result in hallucination_results:\n",
    "        word = result['word']\n",
    "        prob = result['prob']\n",
    "\n",
    "        start = 0\n",
    "        while True:\n",
    "            start = model_output_text.find(word, start)\n",
    "            if start == -1:\n",
    "                break\n",
    "            end = start + len(word)\n",
    "\n",
    "            # save soft_labels\n",
    "            soft_labels.append({\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"prob\": prob\n",
    "            })\n",
    "            start = end\n",
    "\n",
    "    return {\"soft_labels\": soft_labels}\n"
   ],
   "outputs": [],
   "execution_count": 254
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fe707e17-1b23-4cd7-9955-263678640b67",
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.561778Z",
     "start_time": "2025-01-19T08:03:15.558801Z"
    }
   },
   "source": [
    "def triplets_and_references_checker(claims, model_output_text, references, question):\n",
    "    prompt = f\"\"\"\n",
    "   Evaluate hallucinations in the model output text using the question, claims, and references.\n",
    "\n",
    "    ### Question (Model Input)\n",
    "    {question}\n",
    "\n",
    "    ### Claims\n",
    "    {claims}\n",
    "\n",
    "    ### References\n",
    "    {references}\n",
    "\n",
    "    ### Model Output Text\n",
    "    {model_output_text}\n",
    "\n",
    "    ### Instructions\n",
    "    1. Compare each claim with the provided references, question, and existing knowledge.\n",
    "    2. Mark unsupported claims in `model output text` and return hallucinated words with character offsets and probabilities.\n",
    "    3. Assign probabilities based on:\n",
    "    0.7-1.0: Fully fabricated content.\n",
    "    0.4-0.7: Partially incorrect content.\n",
    "    0.1-0.4: Minor inaccuracies.\n",
    "    4. Merge or adjust overlapping hallucinated words appropriately.\n",
    "    5. Include hallucinated words even with low probabilities and return them strictly in JSON format:\n",
    "    [\n",
    "        {{\"word\": <example_word>, \"prob\": <probability>}},\n",
    "     {{\"word\": <another_word>, \"prob\": <probability>}}\n",
    "    ]\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = truncate_text_to_max_tokens(prompt, MAX_TOKENS // 3)\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\",\n",
    "                 \"content\": \"You are an AI assistant for hallucination detection.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "\n",
    "        if not chat_completion.choices or len(chat_completion.choices) == 0:\n",
    "            print(\"Error during hallucination detection: No response choices\")\n",
    "            return {\"soft_labels\": []}\n",
    "\n",
    "        raw_labels = chat_completion.choices[0].message.content\n",
    "\n",
    "        return extract_hallucination_positions(model_output_text, raw_labels)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "        return {\"soft_labels\": []}\n"
   ],
   "outputs": [],
   "execution_count": 255
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aebb245d-9f0d-4514-8e56-9c04a2d7f7ca"
   },
   "source": [
    "## Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0869c823-a9ae-4584-9b8e-e9bd1a2abae9",
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.568569Z",
     "start_time": "2025-01-19T08:03:15.566593Z"
    }
   },
   "source": [
    "def hallucination_detect(question, model_output_text, context):\n",
    "    claims = extract_triplets_to_claims(question, model_output_text)\n",
    "    references = extract_and_get_references(claims, context)\n",
    "    hallucination_results = triplets_and_references_checker(claims, model_output_text, references, question)\n",
    "\n",
    "    soft_labels = hallucination_results.get(\"soft_labels\", [])\n",
    "    hard_labels = recompute_hard_labels(soft_labels)\n",
    "\n",
    "    return soft_labels, hard_labels"
   ],
   "outputs": [],
   "execution_count": 256
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "229008eb-a8d9-4738-ad74-a470376df9c2"
   },
   "source": [
    "## Apply on My Dataset"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.578518Z",
     "start_time": "2025-01-19T08:03:15.573948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "MAX_TOKENS = 16385\n",
    "def truncate_text_to_max_tokens(text, max_tokens):\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "def process_data(question, context, model_output_text, prompt):\n",
    "    \"\"\"\n",
    "    Process data and ensure token limits are respected by truncating text proportionally.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    # Calculate token count for each part\n",
    "    question_tokens = len(tokenizer.encode(question))\n",
    "    context_tokens = len(tokenizer.encode(context))\n",
    "    model_output_tokens = len(tokenizer.encode(model_output_text))\n",
    "    prompt_tokens = len(tokenizer.encode(prompt))\n",
    "\n",
    "    total_tokens = question_tokens + context_tokens + model_output_tokens + prompt_tokens\n",
    "\n",
    "    if total_tokens > MAX_TOKENS:\n",
    "        print(f\"Warning: Total tokens ({total_tokens}) exceed the maximum limit ({MAX_TOKENS}). Adjusting inputs.\")\n",
    "\n",
    "    # Calculate the amount to truncate\n",
    "    excess_tokens = total_tokens - MAX_TOKENS\n",
    "\n",
    "    # Proportional length adjustment for each part\n",
    "    proportion_question = question_tokens / total_tokens\n",
    "    proportion_context = context_tokens / total_tokens\n",
    "    proportion_model_output = model_output_tokens / total_tokens\n",
    "    proportion_prompt = prompt_tokens / total_tokens\n",
    "\n",
    "    # Truncate each part proportionally\n",
    "    question = truncate_text_to_max_tokens(question, int(question_tokens - proportion_question * excess_tokens))\n",
    "    context = truncate_text_to_max_tokens(context, int(context_tokens - proportion_context * excess_tokens))\n",
    "    model_output_text = truncate_text_to_max_tokens(model_output_text, int(model_output_tokens - proportion_model_output * excess_tokens))\n",
    "    prompt = truncate_text_to_max_tokens(prompt, int(prompt_tokens - proportion_prompt * excess_tokens))\n",
    "\n",
    "    # Update token count\n",
    "    total_tokens = len(tokenizer.encode(question)) + len(tokenizer.encode(context)) + len(tokenizer.encode(model_output_text)) + len(tokenizer.encode(prompt))\n",
    "\n",
    "    return question, context, model_output_text, prompt, total_tokens"
   ],
   "outputs": [],
   "execution_count": 257
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.588815Z",
     "start_time": "2025-01-19T08:03:15.587467Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.599464Z",
     "start_time": "2025-01-19T08:03:15.594710Z"
    }
   },
   "source": [
    "# process the dataset and save the results\n",
    "def process_dataset(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    input_files = glob.glob(os.path.join(input_folder, \"*.jsonl\"))\n",
    "\n",
    "    with tqdm(total=len(input_files), desc=\"Processing Files\", unit=\"file\") as file_progress:\n",
    "        for file_path in input_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = [json.loads(line) for line in f]\n",
    "\n",
    "            output_data = []\n",
    "\n",
    "            with tqdm(total=len(data), desc=f\"Processing {os.path.basename(file_path)}\", unit=\"entry\",\n",
    "                      leave=False) as entry_progress:\n",
    "                for entry in data:\n",
    "                    try:\n",
    "\n",
    "                        # question = truncate_text(entry.get(\"model_input\", \"\"), MAX_TOKENS // 3)\n",
    "                        # model_output_text = truncate_text(entry.get(\"model_output_text\", \"\"), MAX_TOKENS // 3)\n",
    "                        # context = truncate_text(entry.get(\"context_googlecse\", \"\"), MAX_TOKENS // 3)\n",
    "\n",
    "                        question = truncate_text_to_max_tokens(entry.get(\"model_input\", \"\"), MAX_TOKENS // 3)\n",
    "                        model_output_text = truncate_text_to_max_tokens(entry.get(\"model_output_text\", \"\"), MAX_TOKENS // 3)\n",
    "                        context = truncate_text_to_max_tokens(entry.get(\"context_googlecse\", \"\"), MAX_TOKENS // 3)\n",
    "\n",
    "                        soft_labels, hard_labels = hallucination_detect(\n",
    "                            question, model_output_text, context\n",
    "                        )\n",
    "\n",
    "                        question = entry.get(\"model_input\", \"\")\n",
    "                        model_output_text = entry.get(\"model_output_text\", \"\")\n",
    "                        context = entry.get(\"context_googlecse\", \"\")\n",
    "\n",
    "                        soft_labels, hard_labels = hallucination_detect(\n",
    "                            question, model_output_text, context\n",
    "                        )\n",
    "\n",
    "                        output_entry = {\n",
    "                            \"id\": entry.get(\"id\"),\n",
    "                            \"lang\": entry.get(\"lang\"),\n",
    "                            \"model_input\": entry.get(\"model_input\"),\n",
    "                            \"model_output_text\": entry.get(\"model_output_text\"),\n",
    "                            \"model_id\": entry.get(\"model_id\"),\n",
    "                            \"soft_labels\": soft_labels,\n",
    "                            \"hard_labels\": hard_labels,\n",
    "                            \"model_output_logits\": entry.get(\"model_output_logits\"),\n",
    "                            \"model_output_tokens\": entry.get(\"model_output_tokens\")\n",
    "                        }\n",
    "\n",
    "                        output_data.append(output_entry)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"OpenAI API Error: {e}\")\n",
    "                        # return []\n",
    "                        continue\n",
    "                    entry_progress.update(1)\n",
    "\n",
    "            output_file = os.path.join(output_folder, os.path.basename(file_path))\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                for item in output_data:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "            file_progress.update(1)\n",
    "            print(f\"Processed and saved: {output_file}\")"
   ],
   "outputs": [],
   "execution_count": 258
  },
  {
   "metadata": {
    "id": "83272c42-9948-4e8a-811c-b6cec8ec2dd0",
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.611507Z",
     "start_time": "2025-01-19T08:03:15.608403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_hallucination_positions(model_output_text, hallucination_results):\n",
    "    print(\"hallucination_results:\", hallucination_results)\n",
    "\n",
    "    json_matches = re.findall(r'\\[\\s*\\{.*?\\}\\s*\\]', hallucination_results, re.DOTALL)\n",
    "\n",
    "    if not json_matches:\n",
    "        print(\"No valid JSON found. Returning empty labels.\")\n",
    "        return {\"soft_labels\": []}\n",
    "\n",
    "    try:\n",
    "        hallucination_results = json.loads(json_matches[0])\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to decode extracted JSON. Error: {e}. Returning empty labels.\")\n",
    "        return {\"soft_labels\": []}\n",
    "\n",
    "    soft_labels = []\n",
    "\n",
    "    # find the position in the original text\n",
    "    for result in hallucination_results:\n",
    "        word = result['word']\n",
    "        prob = result['prob']\n",
    "\n",
    "        start = 0\n",
    "        while True:\n",
    "            start = model_output_text.find(word, start)\n",
    "            if start == -1:\n",
    "                break\n",
    "            end = start + len(word)\n",
    "\n",
    "            # save soft_labels\n",
    "            soft_labels.append({\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"prob\": prob\n",
    "            })\n",
    "            start = end\n",
    "\n",
    "    return {\"soft_labels\": soft_labels}"
   ],
   "outputs": [],
   "execution_count": 259
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1626cda1-7e3b-4587-a048-98588f7be617",
    "outputId": "8daf4e01-05e8-41b5-8a16-3dc46c9939f5",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-01-19T13:48:05.490931Z",
     "start_time": "2025-01-19T08:03:15.622623Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "def get_project_root():\n",
    "    return os.path.dirname(os.getcwd())\n",
    "\n",
    "input_folder = os.path.join(get_project_root(), \"data/exknowledge/\")\n",
    "output_folder = os.path.join(get_project_root(), \"data/detect_gpt/\")\n",
    "\n",
    "print(\"Input Folder Absolute Path:\", input_folder)\n",
    "process_dataset(input_folder, output_folder)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Folder Absolute Path: /Users/wt/SemEvalTask3/NCL-UoR/Jalynn/Method1/data/exknowledge/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   0%|          | 0/10 [00:00<?, ?file/s]\n",
      "Processing mushroom.ar-val.v2.jsonl:   0%|          | 0/50 [00:00<?, ?entry/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"كوريا الجنوبية\", \"prob\": 0.9},\n",
      "    {\"word\": \"16\", \"prob\": 0.8}\n",
      "]\n",
      "```  \n",
      "\n",
      "The hallucinated words in the model output text are \"كوريا الجنوبية\" and \"16\" with estimated probabilities of 0.9 and 0.8, respectively. These words do not align with the provided claim and references, indicating a likelihood of fabricated or partially incorrect content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:   2%|▏         | 1/50 [00:49<40:09, 49.18s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"كوريا الجنوبية\", \"prob\": 0.9}\n",
      "]\n",
      "```\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"هالوسينغ\", \"prob\": 0.9},\n",
      "    {\"word\": \"هيلبي\", \"prob\": 0.8}\n",
      "]\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:   4%|▍         | 2/50 [01:58<48:56, 61.17s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"هالوسينغ\", \"prob\": 1.0},\n",
      "    {\"word\": \"هيلبي\", \"prob\": 1.0}\n",
      "]\n",
      "```\n",
      "hallucination_results: Based on the provided text, it seems that there was an error or incompleteness in the input related to evaluating hallucinations in the model output text. The claims section includes information about a sports stadium in Austria rather than any content that would indicate hallucinations. For accurate evaluation of hallucinations, a different text or relevant information related to hallucinations is needed. If you have a specific text or claim related to hallucinations that you would like to analyze, please provide that for further evaluation.\n",
      "No valid JSON found. Returning empty labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:   6%|▌         | 3/50 [10:41<3:33:05, 272.02s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: Based on the model output text, it appears that there is an issue with the claim provided as it is incomplete and lacks specific information or context. The text mainly consists of repetitive prompts asking for a claim to verify, without a clear statement to evaluate for hallucinations. To accurately detect hallucinations, a complete and coherent claim related to hallucinations or false perceptions is required. If you have specific claims or statements related to hallucinations that you would like me to analyze, please provide them for further evaluation.\n",
      "No valid JSON found. Returning empty labels.\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"تشنسلر\", \"prob\": 0.8},\n",
      "    {\"word\": \"جون ثان\", \"prob\": 0.9},\n",
      "    {\"word\": \"بَينيت\", \"prob\": 0.7}\n",
      "]\n",
      "```  \n",
      "\n",
      "The model output text hallucinated on the words \"تشنسلر\", \"جون ثان\", and \"بَينيت\" with the provided probabilities based on the references and question. These words are not supported by the claims and references provided, indicating a high probability of fabrication in the model output text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:   8%|▊         | 4/50 [12:17<2:35:19, 202.59s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"الكافي\", \"prob\": 0.8},\n",
      "    {\"word\": \"بَينيت\", \"prob\": 0.9}\n",
      "]\n",
      "``` \n",
      "\n",
      "In the model output text, the words \"الكافي\" and \"بَينيت\" are hallucinated with probabilities indicating partially incorrect content and fully fabricated content, respectively. These words do not align with the provided references, question, and existing knowledge.\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"هميلتن\", \"prob\": 0.9},\n",
      "    {\"word\": \"أكادميكل\", \"prob\": 0.8}\n",
      "]\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:  10%|█         | 5/50 [13:27<1:56:04, 154.78s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"هميلتن\", \"prob\": 0.9},\n",
      "    {\"word\": \"أكادميكل\", \"prob\": 0.8}\n",
      "]\n",
      "```\n",
      "hallucination_results: The model output text seems to have repeated a set of responses in the Arabic language related to not receiving a specific claim to verify. There is no direct evaluation of hallucinations within the provided information. If you have a specific claim or information related to hallucinations that you would like to evaluate, please provide it, and I will be happy to assist you.\n",
      "No valid JSON found. Returning empty labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:  12%|█▏        | 6/50 [21:53<3:21:00, 274.10s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: It appears that there was a system error in the input text. Could you please rephrase your question or provide the specific claim you would like me to assess for hallucinations? Once you provide the claim or context, I will be able to evaluate it based on the information available. Thank you.\n",
      "No valid JSON found. Returning empty labels.\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"مونغا\", \"prob\": 0.9},\n",
      "    {\"word\": \"يُلعب\", \"prob\": 0.7},\n",
      "    {\"word\": \"يلَعب\", \"prob\": 0.8}\n",
      "]\n",
      "```  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:  14%|█▍        | 7/50 [25:14<2:59:22, 250.29s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"إديو مونغا\", \"prob\": 0.8},\n",
      "    {\"word\": \"يُعب\", \"prob\": 0.7},\n",
      "    {\"word\": \"بونجا\", \"prob\": 0.6}\n",
      "]\n",
      "```  \n",
      "\n",
      "In the model output text, the words \"إديو مونغا\" hallucinated into \"إديو مونغا\" and \"بونجا.\" The word \"يُعب\" is also a hallucination from the correct word \"يَلعب.\" The probabilities assigned indicate the level of inaccuracy in the output text based on the provided claims and references.\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"دافوس\", \"prob\": 0.9}\n",
      "]\n",
      "``` \n",
      "\n",
      "Explanation:\n",
      "- The model output text contains the hallucinated word \"دافوس\" which is not supported by the provided references. The probability assigned is 0.9 indicating that this word is a fully fabricated content in the output.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:  16%|█▌        | 8/50 [34:38<4:05:00, 350.01s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"دافوس\", \"prob\": 0.9}\n",
      "]\n",
      "```\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"بطريرك\", \"prob\": 0.8},\n",
      "    {\"word\": \"قسطنطينية\", \"prob\": 0.9},\n",
      "    {\"word\": \"1334.\", \"prob\": 0.6}\n",
      "]\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:  18%|█▊        | 9/50 [1:10:26<10:23:12, 912.01s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[]\n",
      "```\n",
      "No valid JSON found. Returning empty labels.\n",
      "OpenAI API Error: Request timed out.\n",
      "OpenAI API Error: sequence item 51: expected str instance, list found\n",
      "OpenAI API Error: Request timed out.\n",
      "OpenAI API Error: sequence item 48: expected str instance, list found\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"جوائز\", \"prob\": 0.9}\n",
      "]\n",
      "```  \n",
      "\n",
      "In the model output text, the word \"جوائز\" (awards) is hallucinated as it was not mentioned in the claims or references provided. The probability assigned to this hallucinated word is 0.9, indicating it is a fully fabricated content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:  20%|██        | 10/50 [2:29:18<23:14:20, 2091.50s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"جوائز\", \"prob\": 0.8},\n",
      "    {\"word\": \"هوگو\", \"prob\": 0.9}\n",
      "]\n",
      "```  \n",
      "\n",
      "The model output text contains hallucinated words \"جوائز\" and \"هوگو\" with probabilities of 0.8 and 0.9 respectively. These words were not supported by the references provided and the content is considered partly incorrect.\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"الكاش\", \"prob\": 0.9},\n",
      "    {\"word\": \"بيلي\", \"prob\": 0.8}\n",
      "]\n",
      "``` \n",
      "\n",
      "Explanation:\n",
      "- \"الكاش\" is hallucinated as the correct name is \"كاش\" based on the references.\n",
      "- \"بيلي\" is hallucinated, and the correct name is \"جوني\" based on the provided references.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:  22%|██▏       | 11/50 [2:42:13<18:17:35, 1688.60s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"الكاش\", \"prob\": 0.9},\n",
      "    {\"word\": \"بيلي هيل\", \"prob\": 0.8}\n",
      "]\n",
      "```  \n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"هوففليشيم\", \"prob\": 0.9}\n",
      "]\n",
      "``` \n",
      "\n",
      "The word \"هوففليشيم\" in the model output text is hallucinated with a probability of 0.9 as it does not match the correct term \"هوففلسهايم\" based on the reference provided.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:  24%|██▍       | 12/50 [2:56:35<15:10:07, 1437.03s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"هوففليشيم\", \"prob\": 0.9},\n",
      "    {\"word\": \"تقع\", \"prob\": 0.6}\n",
      "]\n",
      "```\n",
      "OpenAI API Error: Request timed out.\n",
      "OpenAI API Error: sequence item 59: expected str instance, list found\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"منطقة\", \"prob\": 0.8},\n",
      "    {\"word\": \"فالكاوو\", \"prob\": 0.9},\n",
      "    {\"word\": \"مترًا\", \"prob\": 0.7}\n",
      "]\n",
      "```  \n",
      "\n",
      "The model output text contains hallucinated words \"منطقة\", \"فالكاوو\", and \"مترًا\" with associated probabilities as mentioned above. These words do not align with the original claims and references provided.\n",
      "OpenAI API Error: Request timed out.\n",
      "OpenAI API Error: sequence item 45: expected str instance, list found\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"يوم الأحد\", \"prob\": 0.9}\n",
      "]\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:  26%|██▌       | 13/50 [3:34:01<17:17:21, 1682.20s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"يوم\", \"prob\": 1.0},\n",
      "    {\"word\": \".\", \"prob\": 1.0}\n",
      "]\n",
      "```\n",
      "OpenAI API Error: Request timed out.\n",
      "OpenAI API Error: Request timed out.\n",
      "OpenAI API Error: sequence item 32: expected str instance, list found\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"PHP\", \"prob\": 0.8},\n",
      "    {\"word\": \"مبرمج\", \"prob\": 0.9},\n",
      "    {\"word\": \"ومؤسس\", \"prob\": 0.7}\n",
      "]\n",
      "```  \n",
      "\n",
      "In the model output text, the words \"PHP\", \"مبرمج\" (programmer), and \"ومؤسس\" (and founder) are hallucinated with probabilities indicating partially incorrect or fully fabricated content. These words were not supported by the references provided or the given claims and question.\n",
      "OpenAI API Error: Request timed out.\n",
      "OpenAI API Error: sequence item 129: expected str instance, list found\n",
      "OpenAI API Error: Request timed out.\n",
      "OpenAI API Error: sequence item 67: expected str instance, list found\n",
      "OpenAI API Error: Request timed out.\n",
      "OpenAI API Error: sequence item 19: expected str instance, list found\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"Blue\", \"prob\": 1.0}\n",
      "]\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.ar-val.v2.jsonl:  28%|██▊       | 14/50 [5:01:46<27:38:31, 2764.20s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"الرمزى\", \"prob\": 0.8},\n",
      "    {\"word\": \"النظم\", \"prob\": 0.9},\n",
      "    {\"word\": \"الخاصه\", \"prob\": 0.7}\n",
      "]\n",
      "```  \n",
      "OpenAI API Error: Request timed out.\n",
      "OpenAI API Error: sequence item 49: expected str instance, list found\n",
      "OpenAI API Error: Request timed out.\n",
      "OpenAI API Error: sequence item 52: expected str instance, list found\n",
      "hallucination_results: ```json\n",
      "[\n",
      "    {\"word\": \"بونزونى\", \"prob\": 0.9},\n",
      "    {\"word\": \"بونزا\", \"prob\": 0.8},\n",
      "    {\"word\": \"بوينت ديل ريو\", \"prob\": 0.7}\n",
      "]\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Files:   0%|          | 0/10 [5:44:49<?, ?file/s]                                  \u001B[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPStatusError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/openai/_base_client.py:1043\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1042\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1043\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1044\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m err:  \u001B[38;5;66;03m# thrown on 4xx and 5xx status code\u001B[39;00m\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpx/_models.py:829\u001B[0m, in \u001B[0;36mraise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    828\u001B[0m decoder \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_content_decoder()\n\u001B[0;32m--> 829\u001B[0m chunker \u001B[38;5;241m=\u001B[39m ByteChunker(chunk_size\u001B[38;5;241m=\u001B[39mchunk_size)\n\u001B[1;32m    830\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request):\n",
      "\u001B[0;31mHTTPStatusError\u001B[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[260], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m output_folder \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(get_project_root(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/detect_gpt/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput Folder Absolute Path:\u001B[39m\u001B[38;5;124m\"\u001B[39m, input_folder)\n\u001B[0;32m---> 10\u001B[0m \u001B[43mprocess_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_folder\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[258], line 35\u001B[0m, in \u001B[0;36mprocess_dataset\u001B[0;34m(input_folder, output_folder)\u001B[0m\n\u001B[1;32m     32\u001B[0m model_output_text \u001B[38;5;241m=\u001B[39m entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_output_text\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     33\u001B[0m context \u001B[38;5;241m=\u001B[39m entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontext_googlecse\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 35\u001B[0m soft_labels, hard_labels \u001B[38;5;241m=\u001B[39m \u001B[43mhallucination_detect\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_output_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m output_entry \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m: entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlang\u001B[39m\u001B[38;5;124m\"\u001B[39m: entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlang\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_output_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_output_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     49\u001B[0m }\n\u001B[1;32m     51\u001B[0m output_data\u001B[38;5;241m.\u001B[39mappend(output_entry)\n",
      "Cell \u001B[0;32mIn[256], line 3\u001B[0m, in \u001B[0;36mhallucination_detect\u001B[0;34m(question, model_output_text, context)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mhallucination_detect\u001B[39m(question, model_output_text, context):\n\u001B[1;32m      2\u001B[0m     claims \u001B[38;5;241m=\u001B[39m extract_triplets_to_claims(question, model_output_text)\n\u001B[0;32m----> 3\u001B[0m     references \u001B[38;5;241m=\u001B[39m \u001B[43mextract_and_get_references\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclaims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     hallucination_results \u001B[38;5;241m=\u001B[39m triplets_and_references_checker(claims, model_output_text, references, question)\n\u001B[1;32m      6\u001B[0m     soft_labels \u001B[38;5;241m=\u001B[39m hallucination_results\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msoft_labels\u001B[39m\u001B[38;5;124m\"\u001B[39m, [])\n",
      "Cell \u001B[0;32mIn[253], line 4\u001B[0m, in \u001B[0;36mextract_and_get_references\u001B[0;34m(claims, context)\u001B[0m\n\u001B[1;32m      2\u001B[0m references \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m claim \u001B[38;5;129;01min\u001B[39;00m claims:\n\u001B[0;32m----> 4\u001B[0m     verified_reference \u001B[38;5;241m=\u001B[39m \u001B[43mget_reference_for_claim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclaim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     references\u001B[38;5;241m.\u001B[39mappend(verified_reference)\n\u001B[1;32m      7\u001B[0m final_reference \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(references) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m context\n",
      "Cell \u001B[0;32mIn[252], line 11\u001B[0m, in \u001B[0;36mget_reference_for_claim\u001B[0;34m(claim)\u001B[0m\n\u001B[1;32m      2\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124mPlease expand, provide additional relevant factual information and verify the following claim:\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124mClaims: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclaim\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124mIf the claim is inaccurate or incomplete, return a corrected, more detailed statement.\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 11\u001B[0m     chat_completion \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msystem\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mYou are an AI assistant verifying claims.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgpt-3.5-turbo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m chat_completion\u001B[38;5;241m.\u001B[39mchoices \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(chat_completion\u001B[38;5;241m.\u001B[39mchoices) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo response for the prompt: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprompt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 279\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:859\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    818\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    819\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    856\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[1;32m    857\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m    858\u001B[0m     validate_response_format(response_format)\n\u001B[0;32m--> 859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maudio\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    866\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    867\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    871\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_completion_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    872\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    873\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodalities\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprediction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreasoning_effort\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    880\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    881\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    882\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mservice_tier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    883\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    884\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    885\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    886\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    887\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    888\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    889\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    891\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    892\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    894\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    895\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    896\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    897\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    898\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    899\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    900\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    901\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/openai/_base_client.py:1283\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1270\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1271\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1278\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1279\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1280\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1281\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1282\u001B[0m     )\n\u001B[0;32m-> 1283\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/openai/_base_client.py:960\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    957\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    958\u001B[0m     retries_taken \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 960\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    961\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    962\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    963\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    964\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    965\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    966\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/openai/_base_client.py:1049\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1047\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m remaining_retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[1;32m   1048\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m-> 1049\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1050\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1051\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1052\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1053\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresponse_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1054\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1055\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1056\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1058\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[1;32m   1059\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[1;32m   1060\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/openai/_base_client.py:1096\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1092\u001B[0m log\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetrying request to \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m in \u001B[39m\u001B[38;5;132;01m%f\u001B[39;00m\u001B[38;5;124m seconds\u001B[39m\u001B[38;5;124m\"\u001B[39m, options\u001B[38;5;241m.\u001B[39murl, timeout)\n\u001B[1;32m   1094\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[1;32m   1095\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[0;32m-> 1096\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m   1099\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   1100\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1103\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1104\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 260
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j19zkiJaJZPJ"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ade0f719-521a-462b-8812-19156817aba5",
    "ExecuteTime": {
     "end_time": "2025-01-19T13:48:05.503299Z",
     "start_time": "2025-01-17T07:16:34.129246Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from scorer import load_jsonl_file_to_records, score_iou, score_cor, main, recompute_hard_labels\n",
    "import argparse as ap\n",
    "import ast"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7e0884a4-b040-4a29-a25c-9e209a2d10d5",
    "outputId": "03d4bbbb-6a4f-4958-f361-d71c127b8838",
    "ExecuteTime": {
     "end_time": "2025-01-19T13:48:05.504089Z",
     "start_time": "2025-01-17T07:16:34.171122Z"
    }
   },
   "source": [
    "def evaluate_iou_and_cor(val_dir, detect_dir, output_file):\n",
    "    \"\"\"\n",
    "    Evaluate IoU and Spearman correlation between the reference (val) and detected (detect) files.\n",
    "\n",
    "    :param val_dir: Directory containing the ground truth files (e.g., data/val/val/)\n",
    "    :param detect_dir: Directory containing the detected files (e.g., data/detect/)\n",
    "    :param output_file: Path to save the evaluation results (optional)\n",
    "    \"\"\"\n",
    "    # List all files in the validation directory\n",
    "    val_files = os.listdir(val_dir)\n",
    "    detect_files = os.listdir(detect_dir)\n",
    "\n",
    "    # Ensure that we are comparing the same files (same lang)\n",
    "    for val_file in val_files:\n",
    "        # Skip non-JSONL files\n",
    "        if not val_file.endswith('.jsonl'):\n",
    "            continue\n",
    "\n",
    "        # Check if the corresponding detect file exists\n",
    "        detect_file_path = os.path.join(detect_dir, val_file)\n",
    "\n",
    "        if not os.path.exists(detect_file_path):\n",
    "            print(f\"Warning: {detect_file_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load ground truth (val) and detected (detect) data\n",
    "        ref_dicts = load_jsonl_file_to_records(os.path.join(val_dir, val_file))\n",
    "        pred_dicts = load_jsonl_file_to_records(detect_file_path)\n",
    "\n",
    "        # Calculate IoU and Spearman correlation\n",
    "        try:\n",
    "            ious, cors = main(ref_dicts, pred_dicts)\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError occurred for file: {val_file}, skipping this file. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Print or save the results\n",
    "        print(f\"Results for {val_file}:\")\n",
    "        print(f\"  Mean IoU: {ious.mean():.8f}\")\n",
    "        print(f\"  Mean Spearman Correlation: {cors.mean():.8f}\")\n",
    "\n",
    "        # Optionally, save the results to a file\n",
    "        if output_file:\n",
    "            with open(output_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(f\"Results for {val_file}:\\n\")\n",
    "                f.write(f\"  Mean IoU: {ious.mean():.8f}\\n\")\n",
    "                f.write(f\"  Mean Spearman Correlation: {cors.mean():.8f}\\n\\n\")\n",
    "\n",
    "\n",
    "val_dir = 'data/val/val/'\n",
    "detect_dir = 'data/detect_gpt/'\n",
    "output_file = 'evaluation_results_gpt.txt'\n",
    "evaluate_iou_and_cor(val_dir, detect_dir, output_file)"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/val/val/'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[65], line 53\u001B[0m\n\u001B[1;32m     51\u001B[0m detect_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/val/detect_2/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     52\u001B[0m output_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevaluation_results3.txt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 53\u001B[0m \u001B[43mevaluate_iou_and_cor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdetect_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[65], line 10\u001B[0m, in \u001B[0;36mevaluate_iou_and_cor\u001B[0;34m(val_dir, detect_dir, output_file)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mEvaluate IoU and Spearman correlation between the reference (val) and detected (detect) files.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03m:param output_file: Path to save the evaluation results (optional)\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# List all files in the validation directory\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m val_files \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m detect_files \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mlistdir(detect_dir)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Ensure that we are comparing the same files (same lang)\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/val/val/'"
     ]
    }
   ],
   "execution_count": 65
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMTMSFVvdGQd2R8L2Q0cnpL",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
