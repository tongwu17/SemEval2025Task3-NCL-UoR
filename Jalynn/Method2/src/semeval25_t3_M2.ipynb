{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5DW1dH2ylAz"
   },
   "source": [
    "# Hallucination Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w26wYl8ar0z4"
   },
   "source": [
    "1. Extractor Keywords\n",
    "   1. Remove Stop Words:\n",
    "      1. Chinese (zh): jieba\n",
    "      2. Arabic (ar): Hugging Face (asafaya/bert-base-arabic)\n",
    "      3. Hindi (hi): indic-nlp-library\n",
    "      4. Basque (eu): tokenize - xx_ent_wiki_sm, Snowball - stopwords-eu.txt\n",
    "      5. Czech (cs): stopwordsios\n",
    "      6. Farsi (fa): Hazm\n",
    "      7. Other Languages: spaCy model\n",
    "   2. Recognize NER Entities:\n",
    "      1. Hugging Face Models:\n",
    "         1. Arabic (ar): asafaya/bert-base-arabic\n",
    "         2. Catalan (ca): projecte-aina/roberta-base-ca-v2-cased-ner\n",
    "         3. Farsi (fa): HooshvareLab/bert-fa-base-uncased-ner-arman\n",
    "         4. Other Languages: FacebookAI/xlm-roberta-large-finetuned-conll03-english\n",
    "      3. For Unrecognized Content, Perform Tokenization (Extract Key Nouns if Possible):\n",
    "         1. Chinese (zh): jieba (tfidf-keywords)\n",
    "         2. Hindi (hi): indic_tokenize\n",
    "         3. Arabic (ar): Hugging Face (asafaya/bert-base-arabic)\n",
    "         4. Czech (cs): Stanza\n",
    "         5. Farsi (fa): Stanza\n",
    "         6. Other Languages: spaCy tokenize\n",
    "\n",
    "2. Acquire External Knowledge:\n",
    "  1. Use Baidu Translate API to translate all extracted key phrases into English as a fallback mechanism for retrieval.\n",
    "  2. Retrieval Rollback Mechanism:\n",
    "First, use the key phrases in the target language to search via the Wikipedia API.\n",
    "    1. If the search fails, use the translated English phrases for retrieval.\n",
    "Note: During retrieval, there might be errors due to Traditional Chinese redirects. These need to be cleared, and results in Traditional Chinese should be forcefully converted.\n",
    "  3. Extract the first 200 characters from the search results.\n",
    "  \n",
    "3. Use (model_input, model_output_text, context) to detect hallucination words and their probabilities via GPT-3.5.\n",
    "\n",
    "4. Merge overlapping words and compute their probabilities using Exponentiation.\n",
    "\n",
    "5. Create Soft Labels: Identify hallucination word positions in the model_output_text and combine them with their computed probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9059ef3-3823-43d9-98af-6e00cb192ed8"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "import httpx\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from scorer import recompute_hard_labels, load_jsonl_file_to_records, score_iou, score_cor, main\n",
    "import numpy as np\n",
    "from langdetect import detect, LangDetectException\n",
    "import re\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae31781a-f87b-4c25-8193-394d3df3d9f3"
   },
   "outputs": [],
   "source": [
    "# set OpenAI API and proxies\n",
    "api_key = \"\"\n",
    "\n",
    "proxies = {\n",
    "    \"http\": \"http://127.0.0.1:10809\",\n",
    "    \"https\": \"http://127.0.0.1:10809\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e2dda00-3efb-41d4-8a62-13fb8ce288df"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an AI model output evaluation expert, responsible for detecting hallucinated words in model output and assigning accurate probability scores to each hallucination.\n",
    "\n",
    "Below is the input information:\n",
    "- **Language**: {language} (e.g., en(English), ar(Arabic), es(Spanish), etc.)\n",
    "- **Question**: {question}\n",
    "- **Model Output**: {output}\n",
    "- **Background Knowledge** (if available): {context}\n",
    "\n",
    "### **Task**:\n",
    "Your task is to:\n",
    "1. **Identify hallucinated words or phrases** in the model output based on the question and background knowledge.\n",
    "   - A word or phrase is considered a hallucination if it:\n",
    "     - Contradicts the background knowledge.\n",
    "     - Is unverifiable or fabricated.\n",
    "     - Contains logical inconsistencies.\n",
    "2. **Assign a probability score** to each hallucinated word or phrase according to the following criteria:\n",
    "   - **Probability > 0.7**: Severe factual errors or contradictions.\n",
    "   - **Probability 0.5 - 0.7**: Unverifiable or speculative content.\n",
    "   - **Probability 0.3 - 0.5**: Minor inconsistencies or unverifiable details.\n",
    "   - **Probability 0.1 - 0.3**: Minor inaccuracies or vague ambiguities.\n",
    "   - **Do not label words with probability â‰¤ 0.1** (i.e., verifiable facts).\n",
    "\n",
    "### **Additional Instructions**:\n",
    "- Do **not** mark redundant or overly generic words (e.g., \"the\", \"a\", \"and\") as hallucinations unless they introduce factual errors.\n",
    "- Pay special attention to:\n",
    "  - **Numerical data** (e.g., dates, quantities, percentages).\n",
    "  - **Named entities** (e.g., people, organizations, locations).\n",
    "  - **Logical contradictions** (e.g., self-contradictions within the text).\n",
    "- If background knowledge is absent, base your judgment solely on internal consistency.\n",
    "\n",
    "### **Example**:\n",
    "#### Input:\n",
    "- **Question**: \"What year did Einstein win the Nobel Prize?\"\n",
    "- **Model Output**: \"Einstein won the Nobel Prize in Physics in 1922 for his discovery of the photoelectric effect.\"\n",
    "- **Background Knowledge**: \"Einstein won the Nobel Prize in Physics in 1921.\"\n",
    "\n",
    "#### Output:\n",
    "[\n",
    "    {{\"word\": \"1922\", \"prob\": 0.9}}\n",
    "]\n",
    "\n",
    "### **Output Format**:\n",
    "Return the result as a JSON array:\n",
    "[\n",
    "    {{\"word\": <example_word>, \"prob\": <probability>}},\n",
    "    {{\"word\": <another_word>, \"prob\": <probability>}}\n",
    "]\n",
    "\n",
    "### Important:\n",
    "- Provide precise word-level annotations.\n",
    "- Do not include any text or explanations outside the JSON array.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1f1b5a7-5cae-4720-bb34-8a0ac6726511"
   },
   "outputs": [],
   "source": [
    "def evaluate_with_selfcheck(question, output, context=\"\", language=\"en\", n=5, retries=3):\n",
    "\n",
    "    if context is None:\n",
    "        context = \"\"\n",
    "\n",
    "    language = language.lower()\n",
    "\n",
    "    prompt = prompt_template.format(question=question, output=output, context=context, language=language)\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": \"gpt-3.5-turbo\",\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"n\": n\n",
    "                },\n",
    "                proxies=proxies\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "                try:\n",
    "                    return json.loads(content)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Failed to parse JSON content: {content}. Error: {e}\")\n",
    "                    return []\n",
    "            else:\n",
    "                print(f\"Request failed with status code: {response.status_code}\")\n",
    "                print(f\"Response: {response.text}\")\n",
    "                return []\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            return []\n",
    "\n",
    "    print(\"Retry limit exceeded, returning empty result\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "724b9954-9519-4991-9346-4c178e035888"
   },
   "outputs": [],
   "source": [
    "# Locate word positions in the original text\n",
    "def locate_word_positions(words_with_probs, model_output_text):\n",
    "    ranges = []\n",
    "    for item in words_with_probs:\n",
    "        word = item[\"word\"]\n",
    "        prob = item[\"prob\"]\n",
    "        start_idx = model_output_text.find(word)\n",
    "        while start_idx != -1:\n",
    "            end_idx = start_idx + len(word)\n",
    "            ranges.append((start_idx, end_idx, prob))\n",
    "            start_idx = model_output_text.find(word, end_idx)\n",
    "    return ranges\n",
    "\n",
    "# Merge overlapping ranges\n",
    "def merge_ranges(ranges):\n",
    "    if not ranges:\n",
    "        return []\n",
    "    # Sort ranges by start position\n",
    "    ranges.sort(key=lambda x: x[0])\n",
    "    merged = [ranges[0]]\n",
    "    for current in ranges[1:]:\n",
    "        last = merged[-1]\n",
    "        if current[0] <= last[1]:  # Overlapping\n",
    "            new_end = max(last[1], current[1])\n",
    "            new_prob = (last[2] + current[2]) / 2  # Average probabilities\n",
    "            merged[-1] = (last[0], new_end, new_prob)\n",
    "        else:\n",
    "            merged.append(current)\n",
    "    return merged\n",
    "\n",
    "# Compute average probabilities with enhanced overlap weighting\n",
    "def compute_average_probability_v3(merged_ranges, all_ranges):\n",
    "    avg_probs = []\n",
    "    for m_start, m_end, _ in merged_ranges:\n",
    "        total_prob = 0\n",
    "        total_overlap_weight = 0\n",
    "\n",
    "        for r_start, r_end, prob in all_ranges:\n",
    "            # Calculate overlap length\n",
    "            overlap_start = max(m_start, r_start)\n",
    "            overlap_end = min(m_end, r_end)\n",
    "            overlap_length = max(0, overlap_end - overlap_start)\n",
    "\n",
    "            # Add weighted contribution (consider overlap frequency)\n",
    "            if overlap_length > 0:\n",
    "                weight = overlap_length  # Base weight is overlap length\n",
    "                total_prob += prob * weight\n",
    "                total_overlap_weight += weight\n",
    "\n",
    "        # Adjust probability by total weight (with enhancement factor)\n",
    "        if total_overlap_weight > 0:\n",
    "            final_prob = (total_prob / total_overlap_weight) ** 1.2  # Enhancing frequent overlaps\n",
    "        else:\n",
    "            final_prob = 0  # No overlap, probability is zero\n",
    "\n",
    "        avg_probs.append(final_prob)\n",
    "    return avg_probs\n",
    "\n",
    "# Main function to process hallucination detection\n",
    "def process_hallucination_detection(question, model_output_text, context, language):\n",
    "    # Call GPT model to get hallucinated words and probabilities\n",
    "    hallucinations = evaluate_with_selfcheck(question, model_output_text, context, language)\n",
    "    # print(\"Hallucinations detected:\", hallucinations)\n",
    "\n",
    "    # Filter out hallucinations with probability <= 0.1\n",
    "    hallucinations = [item for item in hallucinations if item[\"prob\"] > 0.1]\n",
    "\n",
    "    # Locate hallucination positions in the model output text\n",
    "    hallucination_ranges = locate_word_positions(hallucinations, model_output_text)\n",
    "    # print(\"Hallucination Ranges:\", hallucination_ranges)\n",
    "\n",
    "    # Merge overlapping ranges\n",
    "    merged_ranges = merge_ranges(hallucination_ranges)\n",
    "    # print(\"Merged Ranges:\", merged_ranges)\n",
    "\n",
    "    # Compute final probabilities for merged ranges\n",
    "    final_probabilities = compute_average_probability_v3(merged_ranges, hallucination_ranges)\n",
    "\n",
    "    # Prepare final output\n",
    "    result = []\n",
    "    for i, (start, end, _) in enumerate(merged_ranges):\n",
    "        result.append({\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"prob\": final_probabilities[i]\n",
    "        })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9533e57e-f1f0-4ba3-8c8e-36717a126456"
   },
   "outputs": [],
   "source": [
    "def process_dataset(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    input_files = glob.glob(os.path.join(input_folder, \"*.jsonl\"))\n",
    "\n",
    "    with tqdm(total=len(input_files), desc=\"Processing Files\", unit=\"file\") as file_progress:\n",
    "        for file_path in input_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = [json.loads(line) for line in f]\n",
    "\n",
    "            output_data = []\n",
    "\n",
    "            with tqdm(total=len(data), desc=f\"Processing {os.path.basename(file_path)}\", unit=\"entry\", leave=False) as entry_progress:\n",
    "                for entry in data:\n",
    "                    try:\n",
    "                        question = entry.get(\"model_input\", \"\")\n",
    "                        model_output_text = entry.get(\"model_output_text\", \"\")\n",
    "                        context = entry.get(\"wikipedia_context\", \"\")\n",
    "                        language = entry.get(\"lang\", \"\").lower()\n",
    "\n",
    "                        soft_labels = process_hallucination_detection(\n",
    "                            question, model_output_text, context, language\n",
    "                        )\n",
    "                        hard_labels = recompute_hard_labels(soft_labels)\n",
    "\n",
    "                        output_entry = {\n",
    "                            \"id\": entry.get(\"id\"),\n",
    "                            \"lang\": entry.get(\"lang\"),\n",
    "                            \"model_input\": entry.get(\"model_input\"),\n",
    "                            \"model_output_text\": entry.get(\"model_output_text\"),\n",
    "                            \"model_id\": entry.get(\"model_id\"),\n",
    "                            \"soft_labels\": soft_labels,\n",
    "                            \"hard_labels\": hard_labels,\n",
    "                            \"model_output_logits\": entry.get(\"model_output_logits\"),\n",
    "                            \"model_output_tokens\": entry.get(\"model_output_tokens\")\n",
    "                        }\n",
    "\n",
    "                        output_data.append(output_entry)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing entry {entry.get('id')}: {e}\")\n",
    "\n",
    "                    entry_progress.update(1)\n",
    "\n",
    "            output_file = os.path.join(output_folder, os.path.basename(file_path))\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                for item in output_data:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "            file_progress.update(1)\n",
    "            print(f\"Processed and saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b3bc93a-a6f0-45b5-8048-931fc24a9e45",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_folder = \"../data/detect_val/exknowledge_m2/\"\n",
    "output_folder = \"../data/detect_val/detect_gpt_m2/\"\n",
    "\n",
    "process_dataset(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c63a069-8fff-4ea5-a228-533ef3d816ee"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0e962be-e202-4c29-9c62-31cdbe3565a6"
   },
   "outputs": [],
   "source": [
    "def evaluate_iou_and_cor(val_dir, detect_dir, output_file):\n",
    "    \"\"\"\n",
    "    Evaluate IoU and Spearman correlation between the reference (val) and detected (detect) files.\n",
    "\n",
    "    :param val_dir: Directory containing the ground truth files (e.g., data/val/val/)\n",
    "    :param detect_dir: Directory containing the detected files (e.g., data/detect/)\n",
    "    :param output_file: Path to save the evaluation results (optional)\n",
    "    \"\"\"\n",
    "    # List all files in the validation directory\n",
    "    val_files = os.listdir(val_dir)\n",
    "    detect_files = os.listdir(detect_dir)\n",
    "\n",
    "    # Ensure that we are comparing the same files (same lang)\n",
    "    for val_file in val_files:\n",
    "        # Skip non-JSONL files\n",
    "        if not val_file.endswith('.jsonl'):\n",
    "            continue\n",
    "\n",
    "        # Remove the first 'val/' part from the file path to match the structure of detect directory\n",
    "        detect_file_name = val_file.replace('val/', '')  # Remove 'val/' from the file name\n",
    "\n",
    "        # Check if the corresponding detect file exists\n",
    "        detect_file_path = os.path.join(detect_dir, detect_file_name)\n",
    "\n",
    "        if not os.path.exists(detect_file_path):\n",
    "            print(f\"Warning: {detect_file_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load ground truth (val) and detected (detect) data\n",
    "        ref_dicts = load_jsonl_file_to_records(os.path.join(val_dir, val_file))\n",
    "        pred_dicts = load_jsonl_file_to_records(detect_file_path)\n",
    "\n",
    "        # Calculate IoU and Spearman correlation\n",
    "#        try:\n",
    "        ious, cors = main(ref_dicts, pred_dicts)\n",
    "#        except IndexError as e:\n",
    " #           print(f\"IndexError occurred for file: {val_file}, skipping this file. Error: {e}\")\n",
    "  #          continue\n",
    "\n",
    "        # Print or save the results\n",
    "        print(f\"Results for {val_file}:\")\n",
    "        print(f\"  Mean IoU: {ious.mean():.8f}\")\n",
    "        print(f\"  Mean Spearman Correlation: {cors.mean():.8f}\")\n",
    "\n",
    "        # Optionally, save the results to a file\n",
    "        if output_file:\n",
    "            with open(output_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(f\"Results for {val_file}:\\n\")\n",
    "                f.write(f\"  Mean IoU: {ious.mean():.8f}\\n\")\n",
    "                f.write(f\"  Mean Spearman Correlation: {cors.mean():.8f}\\n\\n\")\n",
    "\n",
    "val_dir = '../data/val/val/'\n",
    "detect_dir = '../data/detect_val/detect_gpt_m2/'\n",
    "output_file = 'evaluation_results.txt'\n",
    "evaluate_iou_and_cor(val_dir, detect_dir, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvQSl54xeDkO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
