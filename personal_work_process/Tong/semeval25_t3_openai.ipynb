{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7c6c379-14f0-4423-9f19-5a2d7efee937"
   },
   "source": [
    "# Hallucination Detection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1189959c-2faa-4b3f-8f5b-b8c41cceaf31",
    "ExecuteTime": {
     "end_time": "2025-01-22T09:04:17.435059Z",
     "start_time": "2025-01-22T09:04:17.429498Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import httpx\n",
    "from RefChecker.refchecker.extractor import extractor_prompts\n",
    "# import RefChecker\n",
    "import numpy as np\n",
    "import spacy\n",
    "from scorer import recompute_hard_labels\n",
    "import glob\n",
    "import re\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "40d80ff5-4707-404f-ae56-e61a5dbab507",
    "ExecuteTime": {
     "end_time": "2025-01-22T09:04:17.504777Z",
     "start_time": "2025-01-22T09:04:17.494089Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"\")\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFOs4S7BY7ui"
   },
   "source": [
    "## 1. Extracting Claims (Extractor) - Each claim is a merger of triple-structured knowledge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f47d942e-0617-40e0-b1a5-1c28f4f6a7d7",
    "ExecuteTime": {
     "end_time": "2025-01-22T09:04:17.516404Z",
     "start_time": "2025-01-22T09:04:17.514408Z"
    }
   },
   "source": [
    "LLM_TRIPLET_EXTRACTION_PROMPT_Q = extractor_prompts.LLM_TRIPLET_EXTRACTION_PROMPT_Q\n",
    "LLM_Triplet_To_Claim_PROMPT_Q = extractor_prompts.LLM_Triplet_To_Claim_PROMPT_Q\n",
    "LLM_CLAIM_EXTRACTION_PROMPT_Q = extractor_prompts.LLM_CLAIM_EXTRACTION_PROMPT_Q"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d59a72f6-e73d-482d-bc9c-e263ef495d27",
    "ExecuteTime": {
     "end_time": "2025-01-22T09:04:17.526997Z",
     "start_time": "2025-01-22T09:04:17.523377Z"
    }
   },
   "source": [
    "def extract_triplets_to_claims(question, model_output_text):\n",
    "    prompt = LLM_CLAIM_EXTRACTION_PROMPT_Q.format(q=question, r=model_output_text)\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI assistant who extracts claims.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "\n",
    "        response_content = chat_completion.choices[0].message.content\n",
    "\n",
    "        if not response_content:\n",
    "            print(f\"No response for the prompt: {prompt}\")\n",
    "            return []\n",
    "\n",
    "        return response_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "        return []\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e78c8d47-15c9-4c68-a4b0-ec200591c5fc"
   },
   "source": [
    "## 2. Obtain Complete References"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1a0a7428-4377-4cbb-9a53-6a0df26d48c0",
    "ExecuteTime": {
     "end_time": "2025-01-22T09:04:17.538906Z",
     "start_time": "2025-01-22T09:04:17.535039Z"
    }
   },
   "source": [
    "def get_reference_for_claim(claim):\n",
    "    prompt = f\"\"\"\n",
    "    Please expand, provide additional relevant factual information and verify the following claim:\n",
    "    Claims: {claim}\n",
    "\n",
    "    If the claim is accurate, return the original claim.\n",
    "    If the claim is inaccurate or incomplete, return a corrected, more detailed statement.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI assistant verifying claims.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "\n",
    "        if not chat_completion.choices or len(chat_completion.choices) == 0:\n",
    "            print(f\"No response for the prompt: {prompt}\")\n",
    "            return []\n",
    "\n",
    "        response_content = chat_completion.choices[0].message.content\n",
    "\n",
    "        if not response_content.strip():\n",
    "            print(f\"No content in the response for the prompt: {prompt}\")\n",
    "            return []\n",
    "\n",
    "        return response_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "        return []\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9735211d-f550-4a22-b52c-fa523d0faf5c",
    "ExecuteTime": {
     "end_time": "2025-01-22T09:04:17.549139Z",
     "start_time": "2025-01-22T09:04:17.546291Z"
    }
   },
   "source": [
    "def extract_and_get_references(claims, context):\n",
    "    references = []\n",
    "    for claim in claims:\n",
    "        verified_reference = get_reference_for_claim(claim)\n",
    "        references.append(verified_reference)\n",
    "\n",
    "    final_reference = \" \".join(references) + \" \" + context\n",
    "\n",
    "    return final_reference"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de5f64f8-c12d-4bd9-a90e-f5c1ab591941"
   },
   "source": [
    "## 3. Validate claims, `model_input`, `model_output_text`, and References (Checker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a03c1421-e879-41c1-b162-eb72fcc801d0"
   },
   "source": [
    "The validation results should be mapped back to the `model_output_text`, marking hallucination positions and probabilities, and outputting them as `soft_labels`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "76c4340a-731e-4048-8449-0e53fa9b0f24",
    "ExecuteTime": {
     "end_time": "2025-01-22T09:04:17.561862Z",
     "start_time": "2025-01-22T09:04:17.558864Z"
    }
   },
   "source": [
    "def extract_hallucination_positions(model_output_text, hallucination_results):\n",
    "    # parse JSON data\n",
    "    try:\n",
    "        hallucination_results = json.loads(hallucination_results)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to decode JSON. Returning empty labels.\")\n",
    "        return {\"soft_labels\": []}\n",
    "\n",
    "    soft_labels = []\n",
    "\n",
    "    # find the position in the original text\n",
    "    for result in hallucination_results:\n",
    "        word = result['word']\n",
    "        prob = result['prob']\n",
    "\n",
    "        start = 0\n",
    "        while True:\n",
    "            start = model_output_text.find(word, start)\n",
    "            if start == -1:\n",
    "                break\n",
    "            end = start + len(word)\n",
    "\n",
    "            # save soft_labels\n",
    "            soft_labels.append({\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"prob\": prob\n",
    "            })\n",
    "            start = end\n",
    "\n",
    "    return {\"soft_labels\": soft_labels}\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fe707e17-1b23-4cd7-9955-263678640b67",
    "ExecuteTime": {
     "end_time": "2025-01-22T09:04:17.568320Z",
     "start_time": "2025-01-22T09:04:17.565004Z"
    }
   },
   "source": [
    "def triplets_and_references_checker(claims, model_output_text, references, question):\n",
    "    prompt = f\"\"\"\n",
    "   Evaluate hallucinations in the model output text using the question, claims, and references.\n",
    "\n",
    "    ### Question (Model Input)\n",
    "    {question}\n",
    "\n",
    "    ### Claims\n",
    "    {claims}\n",
    "\n",
    "    ### References\n",
    "    {references}\n",
    "\n",
    "    ### Model Output Text\n",
    "    {model_output_text}\n",
    "\n",
    "    ### Instructions\n",
    "    1. Compare each claim with the provided references, question, and existing knowledge.\n",
    "    2. Mark unsupported claims in `model output text` and return hallucinated words with character offsets and probabilities.\n",
    "    3. Assign probabilities based on:\n",
    "    0.7-1.0: Fully fabricated content.\n",
    "    0.4-0.7: Partially incorrect content.\n",
    "    0.1-0.4: Minor inaccuracies.\n",
    "    4. Merge or adjust overlapping hallucinated words appropriately.\n",
    "    5. Include hallucinated words even with low probabilities and return them strictly in JSON format:\n",
    "    [\n",
    "        {{\"word\": <example_word>, \"prob\": <probability>}},\n",
    "     {{\"word\": <another_word>, \"prob\": <probability>}}\n",
    "    ]\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = truncate_text_to_max_tokens(prompt, MAX_TOKENS // 3)\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\",\n",
    "                 \"content\": \"You are an AI assistant for hallucination detection.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "\n",
    "        if not chat_completion.choices or len(chat_completion.choices) == 0:\n",
    "            print(\"Error during hallucination detection: No response choices\")\n",
    "            return {\"soft_labels\": []}\n",
    "\n",
    "        raw_labels = chat_completion.choices[0].message.content\n",
    "\n",
    "        return extract_hallucination_positions(model_output_text, raw_labels)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "        return {\"soft_labels\": []}\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aebb245d-9f0d-4514-8e56-9c04a2d7f7ca"
   },
   "source": [
    "## Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0869c823-a9ae-4584-9b8e-e9bd1a2abae9",
    "ExecuteTime": {
     "end_time": "2025-01-22T09:04:17.576372Z",
     "start_time": "2025-01-22T09:04:17.574177Z"
    }
   },
   "source": [
    "def hallucination_detect(question, model_output_text, context):\n",
    "    claims = extract_triplets_to_claims(question, model_output_text)\n",
    "    references = extract_and_get_references(claims, context)\n",
    "    hallucination_results = triplets_and_references_checker(claims, model_output_text, references, question)\n",
    "\n",
    "    soft_labels = hallucination_results.get(\"soft_labels\", [])\n",
    "    hard_labels = recompute_hard_labels(soft_labels)\n",
    "\n",
    "    return soft_labels, hard_labels"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T09:05:31.298903Z",
     "start_time": "2025-01-22T09:04:17.603681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "MAX_TOKENS = 16385\n",
    "def truncate_text_to_max_tokens(text, max_tokens):\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "def process_data(question, context, model_output_text, prompt):\n",
    "    \"\"\"\n",
    "    Process data and ensure token limits are respected by truncating text proportionally.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    # Calculate token count for each part\n",
    "    question_tokens = len(tokenizer.encode(question))\n",
    "    context_tokens = len(tokenizer.encode(context))\n",
    "    model_output_tokens = len(tokenizer.encode(model_output_text))\n",
    "    prompt_tokens = len(tokenizer.encode(prompt))\n",
    "\n",
    "    total_tokens = question_tokens + context_tokens + model_output_tokens + prompt_tokens\n",
    "\n",
    "    if total_tokens > MAX_TOKENS:\n",
    "        print(f\"Warning: Total tokens ({total_tokens}) exceed the maximum limit ({MAX_TOKENS}). Adjusting inputs.\")\n",
    "\n",
    "    # Calculate the amount to truncate\n",
    "    excess_tokens = total_tokens - MAX_TOKENS\n",
    "\n",
    "    # Proportional length adjustment for each part\n",
    "    proportion_question = question_tokens / total_tokens\n",
    "    proportion_context = context_tokens / total_tokens\n",
    "    proportion_model_output = model_output_tokens / total_tokens\n",
    "    proportion_prompt = prompt_tokens / total_tokens\n",
    "\n",
    "    # Truncate each part proportionally\n",
    "    question = truncate_text_to_max_tokens(question, int(question_tokens - proportion_question * excess_tokens))\n",
    "    context = truncate_text_to_max_tokens(context, int(context_tokens - proportion_context * excess_tokens))\n",
    "    model_output_text = truncate_text_to_max_tokens(model_output_text, int(model_output_tokens - proportion_model_output * excess_tokens))\n",
    "    prompt = truncate_text_to_max_tokens(prompt, int(prompt_tokens - proportion_prompt * excess_tokens))\n",
    "\n",
    "    # Update token count\n",
    "    total_tokens = len(tokenizer.encode(question)) + len(tokenizer.encode(context)) + len(tokenizer.encode(model_output_text)) + len(tokenizer.encode(prompt))\n",
    "\n",
    "    return question, context, model_output_text, prompt, total_tokens"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode JSON. Returning empty labels.\n",
      "([], [])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T08:03:15.588815Z",
     "start_time": "2025-01-19T08:03:15.587467Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T09:05:31.368277Z",
     "start_time": "2025-01-22T09:05:31.361156Z"
    }
   },
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_hallucination_positions(model_output_text, hallucination_results):\n",
    "    print(\"hallucination_results:\", hallucination_results)\n",
    "\n",
    "    json_matches = re.findall(r'\\[\\s*\\{.*?\\}\\s*\\]', hallucination_results, re.DOTALL)\n",
    "\n",
    "    if not json_matches:\n",
    "        print(\"No valid JSON found. Returning empty labels.\")\n",
    "        return {\"soft_labels\": []}\n",
    "\n",
    "    try:\n",
    "        hallucination_results = json.loads(json_matches[0].strip('```json').strip('```').strip())\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to decode extracted JSON. Error: {e}. Returning empty labels.\")\n",
    "        return {\"soft_labels\": []}\n",
    "\n",
    "    soft_labels = []\n",
    "\n",
    "    # find the position in the original text\n",
    "    for result in hallucination_results:\n",
    "        word = result['word']\n",
    "        prob = result['prob']\n",
    "\n",
    "        start = 0\n",
    "        while True:\n",
    "            start = model_output_text.find(word, start)\n",
    "            if start == -1:\n",
    "                break\n",
    "            end = start + len(word)\n",
    "\n",
    "            # save soft_labels\n",
    "            soft_labels.append({\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"prob\": prob\n",
    "            })\n",
    "            start = end\n",
    "\n",
    "    return {\"soft_labels\": soft_labels}"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "id": "83272c42-9948-4e8a-811c-b6cec8ec2dd0",
    "ExecuteTime": {
     "end_time": "2025-01-22T09:05:31.428964Z",
     "start_time": "2025-01-22T09:05:31.425912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_hallucination_positions(model_output_text, hallucination_results):\n",
    "    print(\"hallucination_results:\", hallucination_results)\n",
    "\n",
    "    json_matches = re.findall(r'\\[\\s*\\{.*?\\}\\s*\\]', hallucination_results, re.DOTALL)\n",
    "\n",
    "    if not json_matches:\n",
    "        print(\"No valid JSON found. Returning empty labels.\")\n",
    "        return {\"soft_labels\": []}\n",
    "\n",
    "    try:\n",
    "        hallucination_results = json.loads(json_matches[0])\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to decode extracted JSON. Error: {e}. Returning empty labels.\")\n",
    "        return {\"soft_labels\": []}\n",
    "\n",
    "    soft_labels = []\n",
    "\n",
    "    # find the position in the original text\n",
    "    for result in hallucination_results:\n",
    "        word = result['word']\n",
    "        prob = result['prob']\n",
    "\n",
    "        start = 0\n",
    "        while True:\n",
    "            start = model_output_text.find(word, start)\n",
    "            if start == -1:\n",
    "                break\n",
    "            end = start + len(word)\n",
    "\n",
    "            # save soft_labels\n",
    "            soft_labels.append({\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"prob\": prob\n",
    "            })\n",
    "            start = end\n",
    "\n",
    "    return {\"soft_labels\": soft_labels}"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1626cda1-7e3b-4587-a048-98588f7be617",
    "outputId": "8daf4e01-05e8-41b5-8a16-3dc46c9939f5",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-01-22T09:05:31.593355Z",
     "start_time": "2025-01-22T09:05:31.479431Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "def get_project_root():\n",
    "    return os.path.dirname(os.getcwd())\n",
    "\n",
    "input_folder = os.path.join(get_project_root(), \"data/exknowledge/\")\n",
    "output_folder = os.path.join(get_project_root(), \"data/detect_gpt/\")\n",
    "\n",
    "print(\"Input Folder Absolute Path:\", input_folder)\n",
    "process_dataset(input_folder, output_folder)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Folder Absolute Path: /Users/wt/SemEvalTask3/NCL-UoR/data/exknowledge/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 0file [00:00, ?file/s]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j19zkiJaJZPJ"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ade0f719-521a-462b-8812-19156817aba5",
    "ExecuteTime": {
     "end_time": "2025-01-22T09:05:31.620080Z",
     "start_time": "2025-01-22T09:05:31.617919Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from scorer import load_jsonl_file_to_records, score_iou, score_cor, main, recompute_hard_labels\n",
    "import argparse as ap\n",
    "import ast"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7e0884a4-b040-4a29-a25c-9e209a2d10d5",
    "outputId": "03d4bbbb-6a4f-4958-f361-d71c127b8838",
    "ExecuteTime": {
     "end_time": "2025-01-22T09:05:31.677718Z",
     "start_time": "2025-01-22T09:05:31.630416Z"
    }
   },
   "source": [
    "def evaluate_iou_and_cor(val_dir, detect_dir, output_file):\n",
    "    \"\"\"\n",
    "    Evaluate IoU and Spearman correlation between the reference (val) and detected (detect) files.\n",
    "\n",
    "    :param val_dir: Directory containing the ground truth files (e.g., data/val/val/)\n",
    "    :param detect_dir: Directory containing the detected files (e.g., data/detect/)\n",
    "    :param output_file: Path to save the evaluation results (optional)\n",
    "    \"\"\"\n",
    "    # List all files in the validation directory\n",
    "    val_files = os.listdir(val_dir)\n",
    "    detect_files = os.listdir(detect_dir)\n",
    "\n",
    "    # Ensure that we are comparing the same files (same lang)\n",
    "    for val_file in val_files:\n",
    "        # Skip non-JSONL files\n",
    "        if not val_file.endswith('.jsonl'):\n",
    "            continue\n",
    "\n",
    "        # Check if the corresponding detect file exists\n",
    "        detect_file_path = os.path.join(detect_dir, val_file)\n",
    "\n",
    "        if not os.path.exists(detect_file_path):\n",
    "            print(f\"Warning: {detect_file_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load ground truth (val) and detected (detect) data\n",
    "        ref_dicts = load_jsonl_file_to_records(os.path.join(val_dir, val_file))\n",
    "        pred_dicts = load_jsonl_file_to_records(detect_file_path)\n",
    "\n",
    "        # Calculate IoU and Spearman correlation\n",
    "        try:\n",
    "            ious, cors = main(ref_dicts, pred_dicts)\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError occurred for file: {val_file}, skipping this file. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Print or save the results\n",
    "        print(f\"Results for {val_file}:\")\n",
    "        print(f\"  Mean IoU: {ious.mean():.8f}\")\n",
    "        print(f\"  Mean Spearman Correlation: {cors.mean():.8f}\")\n",
    "\n",
    "        # Optionally, save the results to a file\n",
    "        if output_file:\n",
    "            with open(output_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(f\"Results for {val_file}:\\n\")\n",
    "                f.write(f\"  Mean IoU: {ious.mean():.8f}\\n\")\n",
    "                f.write(f\"  Mean Spearman Correlation: {cors.mean():.8f}\\n\\n\")\n",
    "\n",
    "\n",
    "val_dir = 'data/val/val/'\n",
    "detect_dir = 'data/detect_gpt/'\n",
    "output_file = 'evaluation_results_gpt.txt'\n",
    "evaluate_iou_and_cor(val_dir, detect_dir, output_file)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for mushroom.ar-val.v2.jsonl:\n",
      "  Mean IoU: 0.18210507\n",
      "  Mean Spearman Correlation: 0.06644335\n",
      "Warning: data/detect_gpt/mushroom.es-val.v2.jsonl not found, skipping.\n",
      "Warning: data/detect_gpt/mushroom.fr-val.v2.jsonl not found, skipping.\n",
      "Warning: data/detect_gpt/mushroom.de-val.v2.jsonl not found, skipping.\n",
      "Warning: data/detect_gpt/mushroom.it-val.v2.jsonl not found, skipping.\n",
      "Warning: data/detect_gpt/mushroom.hi-val.v2.jsonl not found, skipping.\n",
      "Warning: data/detect_gpt/mushroom.zh-val.v2.jsonl not found, skipping.\n",
      "Warning: data/detect_gpt/mushroom.en-val.v2.jsonl not found, skipping.\n",
      "Warning: data/detect_gpt/mushroom.fi-val.v2.jsonl not found, skipping.\n",
      "Warning: data/detect_gpt/mushroom.sv-val.v2.jsonl not found, skipping.\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMTMSFVvdGQd2R8L2Q0cnpL",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
