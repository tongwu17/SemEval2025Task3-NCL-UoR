{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q60j2QN8CAXY"
   },
   "outputs": [],
   "source": [
    "main_folder = \"\"\n",
    "\n",
    "train_folder   = f\"dataset/train/train\"\n",
    "val_folder     = f\"dataset/val/val\"\n",
    "sample_folder  = f\"dataset/sample/sample\"\n",
    "\n",
    "val_filenames = [\"mushroom.ar-val.v2.jsonl\",\n",
    "                \"mushroom.de-val.v2.jsonl\",\n",
    "                \"mushroom.en-val.v2.jsonl\",\n",
    "                \"mushroom.es-val.v2.jsonl\",\n",
    "                \"mushroom.fi-val.v2.jsonl\",\n",
    "                \"mushroom.fr-val.v2.jsonl\",\n",
    "                \"mushroom.hi-val.v2.jsonl\",\n",
    "                \"mushroom.it-val.v2.jsonl\",\n",
    "                \"mushroom.sv-val.v2.jsonl\",\n",
    "                \"mushroom.zh-val.v2.jsonl\"]\n",
    "\n",
    "N = 5\n",
    "\n",
    "api_key = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjATmj8v6krW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wikipediaapi\n",
    "import json\n",
    "from yake import KeywordExtractor\n",
    "from openai import OpenAI\n",
    "import scorer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_gpt_response(message):\n",
    "\n",
    "  chat_completion = client.chat.completions.create(\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": message,\n",
    "          }\n",
    "      ],\n",
    "      model=\"gpt-4\",\n",
    "  )\n",
    "\n",
    "  response_content = chat_completion.choices[0].message.content\n",
    "  return response_content\n",
    "\n",
    "\n",
    "def retrieve_context_from_wikipedia(keyword):\n",
    "    wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "        user_agent=\"MyPythonApp/1.0 (https://example.com; contact@example.com)\"\n",
    "    )\n",
    "    \n",
    "    page = wiki.page(keyword)\n",
    "    if not page.exists():\n",
    "        return \"\"\n",
    "    return page.summary\n",
    "\n",
    "\n",
    "def merge_ranges(ranges):\n",
    "\n",
    "    ranges = [item for sublist in ranges for item in sublist]\n",
    "    \n",
    "    if not ranges:\n",
    "        return []\n",
    "\n",
    "    # Sort ranges by start index\n",
    "    ranges.sort(key=lambda x: x[0])\n",
    "    merged_ranges = [ranges[0]]\n",
    "\n",
    "    for current in ranges[1:]:\n",
    "        last = merged_ranges[-1]\n",
    "        # Check for overlap or adjacency\n",
    "        if current[0] <= last[1]:  # Overlapping or adjacent\n",
    "            merged_ranges[-1] = (last[0], max(last[1], current[1]))\n",
    "        else:\n",
    "            merged_ranges.append(current)\n",
    "\n",
    "    return merged_ranges\n",
    "    \n",
    "\n",
    "def compute_average_probability(merged_ranges, lists):\n",
    "    avg_probabilities = []\n",
    "\n",
    "    for (ms, me) in merged_ranges:\n",
    "        merged_length = me - ms\n",
    "        total_probability = 0\n",
    "\n",
    "        for list_ranges in lists:\n",
    "            overlap_sum = 0\n",
    "\n",
    "            for (s, e) in list_ranges:\n",
    "                # Compute overlap\n",
    "                overlap = max(0, min(me, e) - max(ms, s))\n",
    "                overlap_sum += overlap\n",
    "\n",
    "            # Add to total probability\n",
    "            total_probability += overlap_sum / merged_length if merged_length > 0 else 0\n",
    "\n",
    "        # Average across all lists\n",
    "        avg_probabilities.append(total_probability / len(lists))\n",
    "\n",
    "    return avg_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0v6Lldt6xmq"
   },
   "outputs": [],
   "source": [
    "def predict(data, kw_extractor):\n",
    "    question = data[\"model_input\"]\n",
    "    answer = data[\"model_output_text\"]\n",
    "    logtis = data[\"model_output_logits\"]\n",
    "\n",
    "    # Extract keywords\n",
    "    keywords = kw_extractor.extract_keywords(question)\n",
    "    print(question)\n",
    "    print(answer)\n",
    "    print(keywords)\n",
    "        \n",
    "    # Get contexts\n",
    "    contexts = []\n",
    "    for keyword, _ in keywords:\n",
    "        context = retrieve_context_from_wikipedia(keyword)\n",
    "        contexts.append(context)\n",
    "    print('---------------------')\n",
    "\n",
    "    # Prepare prompt\n",
    "    combined_context = \"\\n\".join(contexts)\n",
    "    list_of_tokens = str(data[\"model_output_tokens\"])\n",
    "    prompt = f\"\"\"Context: {combined_context}\\nSentence: {answer}\\n\"\"\"\n",
    "    prompt = prompt + f\"\"\"Which tokens in the sentence are not supported by the context above?\\n\"\"\"\n",
    "    prompt = prompt + f\"\"\"Provide the answer in the form of a list of hallucination tokens sperated by '|' without accompanying texts.\"\"\"\n",
    "    \n",
    "    # Ask LLM\n",
    "    all_ranges = []\n",
    "    all_responses = []\n",
    "    all_hall_tokens = []\n",
    "    for n in range(N):\n",
    "        # Get response\n",
    "        response = get_gpt_response(prompt)\n",
    "\n",
    "        # Parse response\n",
    "        hall_tokens = response.split(\"|\")\n",
    "        pred_ranges = []\n",
    "        for hall_token in hall_tokens:\n",
    "            hall_token = hall_token.strip()\n",
    "            pred_index = answer.find(hall_token)\n",
    "\n",
    "            if pred_index >= 0:\n",
    "                pred_ranges.append((pred_index, pred_index+len(hall_token)))\n",
    "\n",
    "        all_ranges.append(pred_ranges)\n",
    "        all_responses.append(response)\n",
    "        all_hall_tokens.append(hall_tokens)\n",
    "        print(f\"{n} prediction:\", pred_ranges)\n",
    "    \n",
    "    # Merge predicted ranges\n",
    "    merged_ranges = merge_ranges(all_ranges)\n",
    "\n",
    "    # Compute average probabilities for merged ranges\n",
    "    average_probabilities = compute_average_probability(merged_ranges, all_ranges)\n",
    "        \n",
    "    # Display Results\n",
    "    soft_labels = []\n",
    "    for i, prob in enumerate(average_probabilities):\n",
    "        print(f\"Range {merged_ranges[i]} Average Probability: {prob:.2f}\")\n",
    "        soft_label = {\n",
    "            'start': merged_ranges[i][0],\n",
    "            'end': merged_ranges[i][1],\n",
    "            'prob': prob,\n",
    "        }\n",
    "        soft_labels.append(soft_label)\n",
    "\n",
    "    return combined_context, all_responses, all_hall_tokens, soft_labels\n",
    "\n",
    "\n",
    "# Init keyword extractor\n",
    "kw_extractor = KeywordExtractor()\n",
    "avg_ious = []\n",
    "avg_cors = []\n",
    "filenames = []\n",
    "\n",
    "for filename in val_filenames:\n",
    "    \n",
    "    file_path = os.path.join(val_folder, filename)\n",
    "    dataset = load_json_data(file_path)\n",
    "\n",
    "    ref_dicts = []\n",
    "    pred_dicts = []\n",
    "\n",
    "    for data in dataset:\n",
    "\n",
    "        answer = data[\"model_output_text\"]\n",
    "        combined_context, all_responses, all_hall_tokens, soft_labels = predict(data, kw_extractor)\n",
    "\n",
    "        # Add soft and hard labels to the datapoint\n",
    "        pred_data = {**data}\n",
    "        pred_data[\"soft_labels\"] = soft_labels\n",
    "        pred_data[\"hard_labels\"] = scorer.recompute_hard_labels(soft_labels)\n",
    "\n",
    "        pred_data[\"text_len\"] = len(answer)\n",
    "        data[\"text_len\"] = len(answer)\n",
    "\n",
    "        pred_data[\"combined_context\"] = combined_context\n",
    "        pred_data[\"all_responses\"] = all_responses\n",
    "        pred_data[\"all_hall_tokens\"] = all_hall_tokens\n",
    "        \n",
    "        pred_dicts.append(pred_data)\n",
    "        ref_dicts.append(data)\n",
    "\n",
    "        \n",
    "        # Show prediction results\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        print(answer)\n",
    "        print(all_hall_tokens)\n",
    "        \n",
    "        print(\"GT:\", data[\"hard_labels\"])\n",
    "        for label in data[\"hard_labels\"]:\n",
    "            print(answer[label[0]:label[1]])\n",
    "\n",
    "        print(\"PD soft\", soft_labels)\n",
    "        print(\"PD hard\", scorer.recompute_hard_labels(soft_labels))\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "        # Evaluate each iter\n",
    "        ious, cors = scorer.main(ref_dicts, pred_dicts)\n",
    "        print(\"Avg IoU: \", sum(ious)/len(ious))\n",
    "        print(\"Avg Cor:\", sum(cors)/len(cors))\n",
    "        print('')\n",
    "\n",
    "    # Evaluate\n",
    "    ious, cors = scorer.main(ref_dicts, pred_dicts)\n",
    "    avg_ious.append(sum(ious)/len(ious))\n",
    "    avg_cors.append(sum(cors)/len(cors))\n",
    "    filenames.append(filename)\n",
    "    print(\"Avg IoU: \", sum(ious)/len(ious))\n",
    "    print(\"Avg Cor:\", sum(cors)/len(cors))\n",
    "    print('')\n",
    "\n",
    "    eval_results = pd.DataFrame.from_dict({\"filename\": filenames, \"iou\": avg_ious, \"cor\":avg_cors})\n",
    "    eval_results.to_csv(\"eval_results.csv\", index=False)\n",
    "\n",
    "    pred_file_path = f\"predictions/{filename}\"\n",
    "    with open(pred_file_path, 'w') as fp:\n",
    "        json.dump(pred_dicts, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "semeval25",
   "language": "python",
   "name": "semeval25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
