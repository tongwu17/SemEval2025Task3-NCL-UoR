{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7c6c379-14f0-4423-9f19-5a2d7efee937"
   },
   "source": [
    "# Hallucination Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGX2vt1wWIet"
   },
   "source": [
    "**The method adopts the concept of RefChecker and is primarily divided into Extractor and Checker components. However, I am unable to directly apply the GitHub method (https://github.com/amazon-science/RefChecker/tree/main/refchecker) due to the following issues:**\n",
    "\n",
    "1. The GitHub implementation relies on the OpenAI API, which requires a proxy for usage in China.\n",
    "2. RefChecker does not achieve the goal of hallucination position detection.\n",
    "3. RefChecker uses its own set of references for comparison, but the references I have are limited, some are inaccessible and just can be the context.\n",
    "\n",
    "**Based on the above, I only use the conceptual framework of RefChecker and some of its key prompts for improvement and optimization. In my method, I primarily use the Anthropic API (Claude).**\n",
    "\n",
    "**My idea of solving the problems:**\n",
    "1. **Claim Extraction:** Claims are extracted by using the Anthropic API to process both `model_output_text` and `model_input`. Each claim is a merged representation derived from a triple-structured knowledge breakdown. (This uses prompts from RefChecker's Extractor component).\n",
    "\n",
    "2. **Primary Reference Acquisition:** Based on the claims obtained in Step 1, I use a self-verification approach. Each claim is processed through the Anthropic API for self-verification to obtain factually consistent statements and even supplemental content. The returned statements are combined with external knowledge I retrieved earlier using the Google API. This forms a complete set of references for subsequent verification checks.\n",
    "\n",
    "3. **Verification of Claims, Model Input, Model Output Text, and References:** The extracted claims, references from Step 2, and `model_input` are verified. The Anthropic API is employed with specific prompts to identify hallucinated words in `model_output_text` and estimate their hallucination probability.\n",
    "\n",
    "4. **Soft Label Generation:** Based on the hallucinated words identified above, their positions in `model_output_text` are mapped. Using the probabilities obtained, a set of `soft_labels` is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1189959c-2faa-4b3f-8f5b-b8c41cceaf31"
   },
   "outputs": [],

   "source": [
    "import json\n",
    "import os\n",
    "import anthropic\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import httpx\n",
    "# download RefChecker\n",
    "from RefChecker.refchecker.extractor import extractor_prompts\n",
    "from scorer import recompute_hard_labels\n",
    "import glob\n",
    "import re"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b83c6c91-48de-46e8-a89d-f8660ca9081f"
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40d80ff5-4707-404f-ae56-e61a5dbab507"
   },
   "outputs": [],
   "source": [
    "os.environ['ANTHROPIC_API_KEY'] = '' # set the Anthropic key\n",

    "\n",
    "# setting proxies (cause in China)\n",
    "proxies = {\n",
    "    \"http\": \"http://127.0.0.1:10809\",\n",
    "    \"https\": \"http://127.0.0.1:10809\",\n",
    "    \"socks5\": \"socks5://127.0.0.1:10808\"  # SOCKS5 proxy\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "session.proxies.update(proxies)\n",
    "\n",
    "# setting Anthropic API key\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFOs4S7BY7ui"
   },
   "source": [
    "## 1. Extracting Claims (Extractor) - Each claim is a merger of triple-structured knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f47d942e-0617-40e0-b1a5-1c28f4f6a7d7"
   },
   "outputs": [],
   "source": [
    "LLM_TRIPLET_EXTRACTION_PROMPT_Q = extractor_prompts.LLM_TRIPLET_EXTRACTION_PROMPT_Q\n",
    "LLM_Triplet_To_Claim_PROMPT_Q = extractor_prompts.LLM_Triplet_To_Claim_PROMPT_Q\n",
    "LLM_CLAIM_EXTRACTION_PROMPT_Q = extractor_prompts.LLM_CLAIM_EXTRACTION_PROMPT_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d59a72f6-e73d-482d-bc9c-e263ef495d27"
   },
   "outputs": [],
   "source": [
    "url_anthropic = \"https://api.anthropic.com/v1/messages\"\n",
    "def extract_triplets_to_claims(question, model_output_text):\n",
    "    prompt = LLM_CLAIM_EXTRACTION_PROMPT_Q.format(q=question, r=model_output_text)\n",
    "    headers = {\n",
    "        \"x-api-key\": api_key,\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"anthropic-version\": \"2023-06-01\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"claude-3-5-haiku-20241022\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0,\n",
    "        \"system\": \"You are an AI assistant, you can help to extract claims from a model-generated response for a question. In addition, you should attribute the claims to the sentences followed by the sentence ids.\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # pass SSL verification\n",
    "        response = requests.post(\n",
    "            url_anthropic,\n",
    "            headers=headers,\n",
    "            json=data,\n",
    "            proxies=proxies,\n",
    "            timeout=30,\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['content'][0]['text'].split('\\n')  # Return a line-separated list of claims.\n",
    "        else:\n",
    "            print(f\"extract_triplets_to_claims Error: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Request failed:\", e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e78c8d47-15c9-4c68-a4b0-ec200591c5fc"
   },
   "source": [
    "## 2. Obtain Complete References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a0a7428-4377-4cbb-9a53-6a0df26d48c0"
   },
   "outputs": [],
   "source": [
    "def get_reference_for_claim(claim):\n",
    "    prompt = f\"\"\"\n",
    "    Please expand, provide additional relevant factual information and verify about the following claim:\n",
    "    Claims: {claim}\n",
    "\n",
    "    If the claim is accurate, not the hallucination and complete, return the original claim.\n",
    "    If the claim is inaccurate, partial or lacking detail, return a corrected, a more detailed and comprehensive factual statement.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"x-api-key\": api_key,\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"anthropic-version\": \"2023-06-01\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"claude-3-5-haiku-20241022\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0,\n",
    "        \"system\": \"You are an AI assistant who checks the factual accuracy of claims and returns corrected references if necessary.\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url_anthropic,\n",
    "            headers=headers,\n",
    "            json=data,\n",
    "            proxies=proxies,\n",
    "            timeout=30,\n",
    "            verify=False\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['content'][0]['text']\n",
    "        else:\n",
    "            print(f\"Error verifying claim: {response.status_code}\")\n",
    "            return claim\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Verification request failed:\", e)\n",
    "        return claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9735211d-f550-4a22-b52c-fa523d0faf5c"
   },
   "outputs": [],
   "source": [
    "def extract_and_get_references(claims, context):\n",
    "    references = []\n",
    "    for claim in claims:\n",
    "        verified_reference = get_reference_for_claim(claim)\n",
    "        references.append(verified_reference)\n",
    "\n",
    "    final_reference = \" \".join(references) + \" \" + context\n",
    "\n",
    "    return final_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de5f64f8-c12d-4bd9-a90e-f5c1ab591941"
   },
   "source": [
    "## 3. Validate claims, `model_input`, `model_output_text`, and References (Checker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a03c1421-e879-41c1-b162-eb72fcc801d0"
   },
   "source": [
    "The validation results should be mapped back to the `model_output_text`, marking hallucination positions and probabilities, and outputting them as `soft_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76c4340a-731e-4048-8449-0e53fa9b0f24"
   },
   "outputs": [],
   "source": [
    "def extract_hallucination_positions(model_output_text, hallucination_results):\n",
    "    # Use RE to extract JSON format data\n",
    "    json_matches = re.findall(r'\\[\\s*\\{.*?\\}\\s*\\]', hallucination_results, re.DOTALL)\n",
    "\n",
    "    if not json_matches:\n",
    "        print(\"No valid JSON found. Returning empty labels.\")\n",
    "        return {\"soft_labels\": []}\n",
    "\n",
    "    try:\n",
    "        hallucination_results = json.loads(json_matches[0])\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to decode extracted JSON. Returning empty labels.\")\n",

    "        return {\"soft_labels\": []}\n",
    "\n",
    "    soft_labels = []\n",
    "\n",
    "    # find the position in the original text\n",
    "    for result in hallucination_results:\n",
    "        word = result['word']\n",
    "        prob = result['prob']\n",
    "\n",
    "        start = 0\n",
    "        while True:\n",
    "            start = model_output_text.find(word, start)\n",
    "            if start == -1:\n",
    "                break\n",
    "            end = start + len(word)\n",
    "\n",
    "            # save soft_labels\n",
    "            soft_labels.append({\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"prob\": prob\n",
    "            })\n",
    "            start = end\n",
    "\n",
    "    return {\"soft_labels\": soft_labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fe707e17-1b23-4cd7-9955-263678640b67"
   },
   "outputs": [],
   "source": [
    "def triplets_and_references_checker(claims, model_output_text, references, question):\n",
    "    prompt = f\"\"\"\n",
    "    Evaluate the model output text for hallucinations by comparing it to the provided references, existed fact, claims, and question (model input). Identify any hallucinated or potentially inaccurate parts in the entire model output text. Highlight the hallucinated word and assign a probability of the hallucination word in the `model output text`.\n",
    "\n",
    "    ### Question (Model Input)\n",
    "    {question}\n",
    "\n",
    "    ### Claims\n",
    "    {claims}\n",
    "\n",
    "    ### References\n",
    "    {references}\n",
    "\n",
    "    ### Model Output Text\n",
    "    {model_output_text}\n",
    "\n",
    "    ### Instructions\n",
    "    1. Compare each claim with the provided references, question and existing fact (internal knowledge).\n",
    "    2. If a claim cannot be fully supported by the references, identify the hallucinated words and mark it to `model output text`.\n",
    "    3. Return character-level offsetss and assign hallucination probabilities.\n",
    "    4. If the claim is fully supported, hallucination should not to be labeled.\n",
    "    5. Assign hallucination probabilities based on the following criteria:\n",
    "       - **0.7 - 1.0**: Fully fabricated or highly speculative content with no supporting evidence.\n",
    "       - **0.4 - 0.7**: Partially incorrect or speculative content, but some evidence supports parts of the claim.\n",
    "       - **0.1 - 0.4**: Minor inaccuracies, such as spelling errors, wrong formatting, or small factual deviations.\n",
    "    6. Ensure that the hallucinated words do not overlap or repeat. If overlapping occurs, merge them or seperate them appropriately.\n",
    "    7. Ensure the words are shown in the `model output text`.\n",
    "    8. Highlight text in `model output text` that could potentially be a hallucination even if not explicitly listed in the claims.\n",
    "    9. Return **all the hallucinated words or phrases** and assign each a hallucination probability (between 0 and 1).\n",
    "    10. Do not filter out hallucinations based on low probability. Return results for any potential hallucination.\n",
    "    11. Do not include any explanations, summaries, or additional text. **Return the JSON list directly.**\n",
    "    12. Ensure all potential hallucinations are listed, even those with probabilities as low as 0.1.\n",
    "\n",
    "    ### Output Example\n",
    "    Only return results with all hallucinated words or phrases and their probability **strictly in the following JSON format**:\n",
    "    [\n",
    "        {{\"word\": <example_word>, \"prob\": <probability>}},\n",
    "        {{\"word\": <another_word>, \"prob\": <probability>}}\n",
    "    ]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"x-api-key\": api_key,\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"anthropic-version\": \"2023-06-01\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"claude-3-5-haiku-20241022\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0,\n",
    "        \"system\": \"You are an AI assistant who checks the factual accuracy of claims and returns position and probability of the hallucination from model output text\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url_anthropic,\n",
    "            headers=headers,\n",
    "            json=data,\n",
    "            proxies=proxies,\n",
    "            timeout=30,\n",
    "            verify=False\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            raw_labels = response.json()['content'][0]['text']\n",
    "            return extract_hallucination_positions(model_output_text, raw_labels)\n",
    "        else:\n",
    "            print(f\"Error during hallucination detection: {response.status_code}\")\n",
    "            return {\"soft_labels\": []}\n",
    "    except requests.RequestException as e:\n",
    "        print(\"Hallucination detection request failed:\", e)\n",
    "        return {\"soft_labels\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aebb245d-9f0d-4514-8e56-9c04a2d7f7ca"
   },
   "source": [
    "## Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0869c823-a9ae-4584-9b8e-e9bd1a2abae9"
   },
   "outputs": [],
   "source": [
    "def hallucination_detect(question, model_output_text, context):\n",
    "    claims = extract_triplets_to_claims(question, model_output_text)\n",
    "    references = extract_and_get_references(claims, context)\n",
    "    hallucination_results = triplets_and_references_checker(claims, model_output_text, references, question)\n",
    "    # extract soft_labels\n",
    "    soft_labels = hallucination_results.get(\"soft_labels\", [])\n",
    "\n",
    "    # compute hard_labels\n",
    "    hard_labels = recompute_hard_labels(soft_labels)\n",
    "\n",
    "    return soft_labels, hard_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "229008eb-a8d9-4738-ad74-a470376df9c2"
   },
   "source": [
    "## Apply on My Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83272c42-9948-4e8a-811c-b6cec8ec2dd0"
   },
   "outputs": [],
   "source": [
    "# process the dataset and save the results\n",
    "def process_dataset(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    input_files = glob.glob(os.path.join(input_folder, \"*.jsonl\"))\n",
    "\n",
    "    with tqdm(total=len(input_files), desc=\"Processing Files\", unit=\"file\") as file_progress:\n",
    "        for file_path in input_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = [json.loads(line) for line in f]\n",
    "\n",
    "            output_data = []\n",
    "\n",
    "            with tqdm(total=len(data), desc=f\"Processing {os.path.basename(file_path)}\", unit=\"entry\", leave=False) as entry_progress:\n",
    "                for entry in data:\n",
    "                    try:\n",
    "                        question = entry.get(\"model_input\", \"\")\n",
    "                        model_output_text = entry.get(\"model_output_text\", \"\")\n",
    "                        context = entry.get(\"context_googlecse\", \"\")\n",
    "\n",
    "                        soft_labels, hard_labels = hallucination_detect(\n",
    "                            question, model_output_text, context\n",
    "                        )\n",
    "\n",
    "                        output_entry = {\n",
    "                            \"id\": entry.get(\"id\"),\n",
    "                            \"lang\": entry.get(\"lang\"),\n",
    "                            \"model_input\": entry.get(\"model_input\"),\n",
    "                            \"model_output_text\": entry.get(\"model_output_text\"),\n",
    "                            \"model_id\": entry.get(\"model_id\"),\n",
    "                            \"soft_labels\": soft_labels,\n",
    "                            \"hard_labels\": hard_labels,\n",
    "                            \"model_output_logits\": entry.get(\"model_output_logits\"),\n",
    "                            \"model_output_tokens\": entry.get(\"model_output_tokens\")\n",
    "                        }\n",
    "\n",
    "                        output_data.append(output_entry)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing entry {entry.get('id')}: {e}\")\n",
    "\n",
    "                    entry_progress.update(1)\n",
    "\n",
    "            output_file = os.path.join(output_folder, os.path.basename(file_path))\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                for item in output_data:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "            file_progress.update(1)\n",
    "            print(f\"Processed and saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1626cda1-7e3b-4587-a048-98588f7be617",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jsonl files with context(Google API)\n",
    "input_folder = \"../data/detect_val/exknowledge_m1/\"\n",
    "# output detected jsonl files\n",
    "output_folder = \"../data/detect_val/detect_m1/\"\n",

    "\n",
    "process_dataset(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j19zkiJaJZPJ"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ade0f719-521a-462b-8812-19156817aba5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from scorer import load_jsonl_file_to_records, score_iou, score_cor, main, recompute_hard_labels\n",
    "import argparse as ap\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e0884a4-b040-4a29-a25c-9e209a2d10d5"
   },
   "outputs": [],

   "source": [
    "def evaluate_iou_and_cor(val_dir, detect_dir, output_file):\n",
    "    \"\"\"\n",
    "    Evaluate IoU and Spearman correlation between the reference (val) and detected (detect) files.\n",
    "\n",
    "    :param val_dir: Directory containing the ground truth files (e.g., data/val/val/)\n",
    "    :param detect_dir: Directory containing the detected files (e.g., data/detect/)\n",
    "    :param output_file: Path to save the evaluation results (optional)\n",
    "    \"\"\"\n",
    "    # List all files in the validation directory\n",
    "    val_files = os.listdir(val_dir)\n",
    "    detect_files = os.listdir(detect_dir)\n",
    "\n",
    "    # Ensure that we are comparing the same files (same lang)\n",
    "    for val_file in val_files:\n",
    "        # Skip non-JSONL files\n",
    "        if not val_file.endswith('.jsonl'):\n",
    "            continue\n",
    "\n",
    "        # Check if the corresponding detect file exists\n",
    "        detect_file_path = os.path.join(detect_dir, val_file)\n",
    "\n",
    "        if not os.path.exists(detect_file_path):\n",
    "            print(f\"Warning: {detect_file_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load ground truth (val) and detected (detect) data\n",
    "        ref_dicts = load_jsonl_file_to_records(os.path.join(val_dir, val_file))\n",
    "        pred_dicts = load_jsonl_file_to_records(detect_file_path)\n",
    "\n",
    "        # Calculate IoU and Spearman correlation\n",
    "        try:\n",
    "            ious, cors = main(ref_dicts, pred_dicts)\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError occurred for file: {val_file}, skipping this file. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Print or save the results\n",
    "        print(f\"Results for {val_file}:\")\n",
    "        print(f\"  Mean IoU: {ious.mean():.8f}\")\n",
    "        print(f\"  Mean Spearman Correlation: {cors.mean():.8f}\")\n",
    "\n",
    "        # Optionally, save the results to a file\n",
    "        if output_file:\n",
    "            with open(output_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(f\"Results for {val_file}:\\n\")\n",
    "                f.write(f\"  Mean IoU: {ious.mean():.8f}\\n\")\n",
    "                f.write(f\"  Mean Spearman Correlation: {cors.mean():.8f}\\n\\n\")\n",
    "\n",
    "val_dir = 'data/val/val/'\n",
    "detect_dir = 'data/val/detect_2/'\n",
    "output_file = 'evaluation_results3.txt'\n",
    "evaluate_iou_and_cor(val_dir, detect_dir, output_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"

  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
